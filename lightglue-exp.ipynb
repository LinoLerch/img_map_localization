{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGlue Based Image-Map Matching\n",
    "this notebook is based on the [LightGlue Demo](https://github.com/cvg/LightGlue/blob/main/demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are on colab: this clones the repo and installs the dependencies\n",
    "from pathlib import Path\n",
    "\n",
    "# if Path.cwd().name != \"LightGlue\":\n",
    "#     if not Path(\"LightGlue\").exists():\n",
    "#         !git clone --quiet https://github.com/cvg/LightGlue/\n",
    "#     %cd LightGlue\n",
    "    # !pip install --progress-bar off --quiet -e .\n",
    "\n",
    "from lightglue import LightGlue, SuperPoint, DISK\n",
    "from lightglue.utils import load_image, rbd\n",
    "from lightglue import viz2d\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "images = Path(\"assets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load extractor and matcher module\n",
    "In this example we use SuperPoint features combined with LightGlue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability and set device appropriately\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")  \n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_num_keypoints=2048\n",
    "max_num_keypoints = None\n",
    "extractor = SuperPoint(max_num_keypoints = max_num_keypoints).eval().to(device)  # load the extractor\n",
    "matcher = LightGlue(features=\"superpoint\").eval().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data\")\n",
    "# Load images and move to device (works for both CUDA and CPU)\n",
    "image_map = load_image(data_path / 'map.png').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training positions\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "train_pos = pd.read_csv(data_path / 'train_data/train_pos.csv')\n",
    "\n",
    "def extract_and_match(image0, image_map, map_features=None):\n",
    "    feats0 = extractor.extract(image0)\n",
    "    if map_features is not None:\n",
    "        feats1 = map_features\n",
    "    else:\n",
    "        feats1 = extractor.extract(image_map)\n",
    "    matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n",
    "    feats0, feats1, matches01 = [\n",
    "        rbd(x) for x in [feats0, feats1, matches01]\n",
    "    ]  # remove batch dimension\n",
    "\n",
    "    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "    # print(f\"__Number of matches: {len(m_kpts0)}\")\n",
    "    return m_kpts0, m_kpts1, matches01, kpts0, kpts1\n",
    "\n",
    "def get_ground_truth_positions(train_pos, img_path):\n",
    "    # Get id from filename\n",
    "    id = int(img_path.name.split('.')[0])\n",
    "    # Get the ground truth positions for the current image\n",
    "    gt_positions = train_pos[train_pos['id'] == id]\n",
    "    gt_x = gt_positions.iloc[0]['x_pixel']\n",
    "    gt_y = gt_positions.iloc[0]['y_pixel']\n",
    "    return gt_x, gt_y, id\n",
    "\n",
    "def get_zoomed_map(image_map, gt_x, gt_y, zoom_factor=5):\n",
    "    # Get map dimensions\n",
    "    map_height, map_width = image_map.shape[1], image_map.shape[2]\n",
    "    \n",
    "    # Calculate zoom window (4x zoom means 1/4 of original size)\n",
    "    window_width = map_width // zoom_factor\n",
    "    window_height = map_height // zoom_factor\n",
    "    \n",
    "    # Calculate crop boundaries centered on ground truth\n",
    "    left = max(0, int(gt_x - window_width // 2))\n",
    "    right = min(map_width, int(gt_x + window_width // 2))\n",
    "    top = max(0, int(gt_y - window_height // 2))\n",
    "    bottom = min(map_height, int(gt_y + window_height // 2))\n",
    "    \n",
    "    # Crop the map\n",
    "    zoomed_map = image_map[:, top:bottom, left:right]\n",
    "\n",
    "    # Adjust ground truth coordinates for the cropped image\n",
    "    adj_gt_x = gt_x - left\n",
    "    adj_gt_y = gt_y - top\n",
    "    \n",
    "    return zoomed_map, adj_gt_x, adj_gt_y\n",
    "\n",
    "def get_map_tile_features(image_map, zoom_factor=5):\n",
    "    \"\"\"\n",
    "    Extract features for map tiles at a specified zoom factor, creating a grid of tiles across the entire map.\n",
    "    \"\"\"\n",
    "    # Get map dimensions\n",
    "    map_height, map_width = image_map.shape[1], image_map.shape[2]\n",
    "    \n",
    "    # Calculate tile dimensions\n",
    "    tile_width = map_width // zoom_factor\n",
    "    tile_height = map_height // zoom_factor\n",
    "    \n",
    "    tile_features = []\n",
    "    tile_positions = []\n",
    "    tiles = []\n",
    "    \n",
    "    # Create grid of tiles\n",
    "    for row in range(zoom_factor):\n",
    "        for col in range(zoom_factor):\n",
    "            # Calculate tile boundaries\n",
    "            left = col * tile_width\n",
    "            right = min((col + 1) * tile_width, map_width)\n",
    "            top = row * tile_height\n",
    "            bottom = min((row + 1) * tile_height, map_height)\n",
    "            \n",
    "            # Extract tile\n",
    "            tile = image_map[:, top:bottom, left:right]\n",
    "            \n",
    "            # Extract features from the tile\n",
    "            feats = extractor.extract(tile)\n",
    "            feats = rbd(feats)  # remove batch dimension\n",
    "            \n",
    "            tile_features.append(feats)\n",
    "            tile_positions.append({\n",
    "                'row': row, \n",
    "                'col': col, \n",
    "                'left': left, \n",
    "                'right': right, \n",
    "                'top': top, \n",
    "                'bottom': bottom\n",
    "            })\n",
    "            tiles.append(tile)\n",
    "    \n",
    "    return tile_features, tile_positions, tiles\n",
    "\n",
    "def plot_matches(image0, img_path, image_map, train_pos, zoom_factor=5, map_features=None, plot_gt=False):\n",
    "    gt_x, gt_y, id = get_ground_truth_positions(train_pos, img_path)\n",
    "    print(f\"__Plotting ground truth for ID: {id}\")\n",
    "    print(f\"__Ground truth position: ({gt_x:.1f}, {gt_y:.1f})\")\n",
    "\n",
    "    # Get zoomed map and adjusted ground truth coordinates\n",
    "    if zoom_factor > 1:\n",
    "        zoomed_map, adj_gt_x, adj_gt_y = get_zoomed_map(image_map, gt_x, gt_y, zoom_factor)\n",
    "        image_map = zoomed_map.to(device)\n",
    "        gt_x, gt_y = adj_gt_x, adj_gt_y\n",
    "\n",
    "    # Extract and match keypoints\n",
    "    m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(image0, image_map, map_features)\n",
    "    # Visualize the images\n",
    "    _ = viz2d.plot_images([image0, image_map], titles=[f\"Image {id}\", f\"Map (Zoom {zoom_factor}x)\"])\n",
    "    # Get the current figure and axes\n",
    "    fig = plt.gcf()\n",
    "    axes = fig.axes\n",
    "    \n",
    "    # Plot red dot on the map (image1) at ground truth position\n",
    "    if plot_gt:\n",
    "        axes[1].plot(gt_x, gt_y, 'ro', markersize=8, markeredgecolor='white', markeredgewidth=1)\n",
    "        axes[1].text(gt_x + 20, gt_y - 20, f'GT ({gt_x:.0f}, {gt_y:.0f})', \n",
    "                    color='red', fontsize=10, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    # Plot matches\n",
    "    viz2d.plot_matches(m_kpts0, m_kpts1, color=\"blue\", lw=0.3)\n",
    "    viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers')\n",
    "    \n",
    "    # Compute center of map matches and plot\n",
    "    if len(m_kpts1) > 0:\n",
    "        center_x = m_kpts1[:, 0].mean().item()\n",
    "        center_y = m_kpts1[:, 1].mean().item()\n",
    "        axes[1].plot(center_x, center_y, 'go', markersize=10, markeredgecolor='white', markeredgewidth=2)\n",
    "        axes[1].text(center_x + 20, center_y + 20, f'Center ({center_x:.0f}, {center_y:.0f})', \n",
    "                    color='green', fontsize=10, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return center_x, center_y\n",
    "\n",
    "    # # Plot pruned keypoints\n",
    "    # kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "    # viz2d.plot_images([image0, image_map])\n",
    "    # viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=6)\n",
    "\n",
    "\n",
    "def compute_pose_prediction(image0, img_path, image_map, train_pos, zoom_factor=5, map_features=None, plot_gt=False):\n",
    "    # Extract and match keypoints\n",
    "    m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(image0, image_map, map_features)\n",
    "    \n",
    "    # Compute center of map matches and plot\n",
    "    if len(m_kpts1) > 0:\n",
    "        center_x = m_kpts1[:, 0].mean().item()\n",
    "        center_y = m_kpts1[:, 1].mean().item()\n",
    "\n",
    "    return center_x, center_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGE_PATH = data_path / \"test_data\" / \"test_images\"\n",
    "TRAIN_IMAGE_PATH = data_path / \"train_data\" / \"train_images\"\n",
    "\n",
    "image_paths = sorted(TRAIN_IMAGE_PATH.iterdir())\n",
    "\n",
    "# Configuration parameters\n",
    "DRONE_SCALE_FACTOR = 8  # Scale factor for drone image resizing\n",
    "MAP_ZOOM_FACTOR = int(DRONE_SCALE_FACTOR * 0.625)     # Map tiling factor\n",
    "STRIDE_FACTOR = 2  # 2 = 50% overlap; 1 = stride equals window size\n",
    "\n",
    "# Known resolutions\n",
    "DRONE_WIDTH = 8004\n",
    "DRONE_HEIGHT = 6001\n",
    "MAP_WIDTH = 5000\n",
    "MAP_HEIGHT = 2500\n",
    "\n",
    "NUM_BEST_TILES = 5\n",
    "\n",
    "# Calculate target resize dimensions\n",
    "target_width = DRONE_WIDTH // DRONE_SCALE_FACTOR   # 8004 // 6 = 1334\n",
    "target_height = DRONE_HEIGHT // DRONE_SCALE_FACTOR # 6001 // 6 = 1000\n",
    "\n",
    "window_height, window_width = MAP_HEIGHT // MAP_ZOOM_FACTOR, MAP_WIDTH // MAP_ZOOM_FACTOR\n",
    "stride_y, stride_x = window_height // STRIDE_FACTOR, window_width // STRIDE_FACTOR\n",
    "\n",
    "print(f\"Drone image scaling: {DRONE_WIDTH}x{DRONE_HEIGHT} -> {target_width}x{target_height} (factor: {DRONE_SCALE_FACTOR})\")\n",
    "print(f\"Map dimensions: {MAP_WIDTH}x{MAP_HEIGHT} -> Tile size: {window_width}x{window_height} (zoom factor: {MAP_ZOOM_FACTOR})\")\n",
    "print(f\"Stride: {stride_x}x{stride_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for x images in train_data with a step of 50\n",
    "for i in range(0, len(image_paths), 70):\n",
    "    img_path = image_paths[i]\n",
    "    print(f\"Processing image: {img_path.name}...\")\n",
    "    \n",
    "    # Resize drone image using calculated dimensions\n",
    "    image0 = load_image(img_path, resize=(target_height, target_width)).to(device)\n",
    "    \n",
    "    # Store tile results with match counts\n",
    "    tile_results = []\n",
    "    \n",
    "    # Generate all 90-degree rotations of the image (0, 90, 180, 270 degrees)\n",
    "    rotated_images = [image0]\n",
    "    for k in range(1, 4):\n",
    "        rotated_images.append(torch.rot90(image0, k, dims=[1, 2]))\n",
    "    for rot_idx, rotated_image in enumerate(rotated_images):\n",
    "\n",
    "        windows = []\n",
    "        map_tile_positions = []\n",
    "\n",
    "        for top in range(0, MAP_HEIGHT - window_height + 1, stride_y):\n",
    "            for left in range(0, MAP_WIDTH - window_width + 1, stride_x):\n",
    "                bottom = top + window_height\n",
    "                right = left + window_width\n",
    "                window = image_map[:, top:bottom, left:right]\n",
    "                windows.append(window)\n",
    "                map_tile_positions.append(\n",
    "                    {\"top\": top,\n",
    "                    \"bottom\": bottom,\n",
    "                    \"left\": left,\n",
    "                    \"right\": right}\n",
    "                    )\n",
    "        \n",
    "        for tile_pos, tile_img in zip(map_tile_positions, windows):\n",
    "            # Extract and match to count matches without plotting\n",
    "            m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(rotated_image, tile_img, map_features=None)\n",
    "            num_matches = len(m_kpts0)\n",
    "            \n",
    "            # Store results\n",
    "            tile_results.append({\n",
    "                'tile_pos': tile_pos,\n",
    "                'tile_img': tile_img,\n",
    "                'num_matches': num_matches,\n",
    "                'matches_data': (m_kpts0, m_kpts1, matches01, kpts0, kpts1),\n",
    "                'rotation_index': rot_idx,\n",
    "                'rotated_image': rotated_image\n",
    "            })\n",
    "            \n",
    "            # print(f\"__Tile [{tile_pos[\"left\"]}, {tile_pos[\"top\"]}] has {num_matches} matches\")\n",
    "        \n",
    "    # Sort by number of matches (descending) and take top 3\n",
    "    best_tiles = sorted(tile_results, key=lambda x: x['num_matches'], reverse=True)[:NUM_BEST_TILES]\n",
    "    \n",
    "    print(f\"\\n=== Top 3 tiles for image {img_path.name} ===\")\n",
    "    for idx, tile_result in enumerate(best_tiles, 1):\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        num_matches = tile_result['num_matches']\n",
    "        print(f\"{idx}. Tile {tile_pos['left']}, {tile_pos['top']}: {num_matches} matches\")\n",
    "    \n",
    "    # Plot only the best 3 tiles\n",
    "    for idx, tile_result in enumerate(best_tiles, 1):\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        tile_img = tile_result['tile_img']\n",
    "        num_matches = tile_result['num_matches']\n",
    "        best_rotation_idx = tile_result['rotation_index']\n",
    "        rotated_image = tile_result['rotated_image']\n",
    "        \n",
    "        print(f\"\\n--- Plotting #{idx} best tile (image rot= {best_rotation_idx *90}) at: [{tile_pos['left']}, {tile_pos['top']}] with {num_matches} matches ---\")\n",
    "        pred_x, pred_y = plot_matches(rotated_image, img_path, tile_img, train_pos, \n",
    "                                      zoom_factor=1, map_features=None, plot_gt=False)\n",
    "        \n",
    "        # Plot whole map with best tile highlighted, predicted position, and ground truth\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.imshow(image_map.cpu().numpy().transpose(1, 2, 0))\n",
    "        \n",
    "        # Get ground truth position for the current image\n",
    "        gt_x, gt_y, id = get_ground_truth_positions(train_pos, img_path)\n",
    "        \n",
    "        # Highlight the best tile with a rectangle\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        rect = plt.Rectangle((tile_pos['left'], tile_pos['top']), \n",
    "                   tile_pos['right'] - tile_pos['left'], \n",
    "                   tile_pos['bottom'] - tile_pos['top'],\n",
    "                   linewidth=1, edgecolor='blue', facecolor='none', alpha=0.8)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Adjust predicted position to full map coordinates\n",
    "        full_map_pred_x = pred_x + tile_pos['left']\n",
    "        full_map_pred_y = pred_y + tile_pos['top']\n",
    "        \n",
    "        # Plot ground truth position (red dot)\n",
    "        ax.plot(gt_x, gt_y, 'ro', markersize=10, markeredgecolor='white', markeredgewidth=2)\n",
    "        ax.text(gt_x + 50, gt_y - 50, f'GT ({gt_x:.0f}, {gt_y:.0f})', \n",
    "            color='red', fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Plot predicted position (green dot)\n",
    "        ax.plot(full_map_pred_x, full_map_pred_y, 'go', markersize=10, markeredgecolor='white', markeredgewidth=2)\n",
    "        ax.text(full_map_pred_x + 50, full_map_pred_y + 50, f'Pred ({full_map_pred_x:.0f}, {full_map_pred_y:.0f})', \n",
    "            color='green', fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Add tile information\n",
    "        ax.text(tile_pos['left'] + 10, tile_pos['top'] + 30, \n",
    "            f'Best Tile \\n{num_matches} matches', \n",
    "            color='blue', fontsize=10, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        ax.set_title(f'Full Map - Image {id} - Tile Ranking #{idx}')\n",
    "        ax.set_xlabel('X coordinate')\n",
    "        ax.set_ylabel('Y coordinate')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on ALL test data and save to CSV\n",
    "TEST_IMAGE_PATH = data_path / \"test_data\" / \"test_images\"\n",
    "test_image_paths = sorted(TEST_IMAGE_PATH.iterdir())\n",
    "MIN_MATCHES_THRESHOLD = 10  # Minimum matches to consider a tile\n",
    "HIGH_CONFIDENCE_THRESHOLD = 40  # If a tile has this many matches, use only this tile\n",
    "print(f\"Processing {len(test_image_paths)} test images...\")\n",
    "\n",
    "# Prepare CSV file with headers\n",
    "output_path = data_path / 'predictions_lightglue_test.csv'\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write('id,x_pixel,y_pixel\\n')\n",
    "\n",
    "for i, img_path in enumerate(test_image_paths):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing {i+1}/{len(test_image_paths)}: {img_path.name}\")\n",
    "    \n",
    "    img_id = int(img_path.name.split('.')[0])\n",
    "    \n",
    "    # Resize drone image using calculated dimensions\n",
    "    image0 = load_image(img_path, resize=(target_height, target_width)).to(device)\n",
    "    \n",
    "    # Store tile results with match counts\n",
    "    tile_results = []\n",
    "    \n",
    "    # Generate all 90-degree rotations of the image (0, 90, 180, 270 degrees)\n",
    "    rotated_images = [image0]\n",
    "    for k in range(1, 4):\n",
    "        rotated_images.append(torch.rot90(image0, k, dims=[1, 2]))\n",
    "    for rot_idx, rotated_image in enumerate(rotated_images):\n",
    "    \n",
    "        # Sliding window approach\n",
    "        windows = []\n",
    "        map_tile_positions = []\n",
    "\n",
    "        for top in range(0, MAP_HEIGHT - window_height + 1, stride_y):\n",
    "            for left in range(0, MAP_WIDTH - window_width + 1, stride_x):\n",
    "                bottom = top + window_height\n",
    "                right = left + window_width\n",
    "                window = image_map[:, top:bottom, left:right]\n",
    "                windows.append(window)\n",
    "                map_tile_positions.append(\n",
    "                    {\"top\": top,\n",
    "                    \"bottom\": bottom,\n",
    "                    \"left\": left,\n",
    "                    \"right\": right}\n",
    "                    )\n",
    "        \n",
    "        for tile_pos, tile_img in zip(map_tile_positions, windows):\n",
    "            # Extract and match to count matches without plotting\n",
    "            m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(rotated_image, tile_img, map_features=None)\n",
    "            num_matches = len(m_kpts0)\n",
    "            \n",
    "            # Store results\n",
    "            tile_results.append({\n",
    "                'tile_pos': tile_pos,\n",
    "                'tile_img': tile_img,\n",
    "                'num_matches': num_matches,\n",
    "                'matches_data': (m_kpts0, m_kpts1, matches01, kpts0, kpts1),\n",
    "                'rotation_index': rot_idx,\n",
    "                'rotated_image': rotated_image\n",
    "            })\n",
    "        \n",
    "     # Sort by number of matches (descending) and apply filtering logic\n",
    "    sorted_tiles = sorted(tile_results, key=lambda x: x['num_matches'], reverse=True)\n",
    "\n",
    "    # Check if any tiles have high confidence\n",
    "    high_confidence_tiles = [tile for tile in sorted_tiles if tile['num_matches'] >= HIGH_CONFIDENCE_THRESHOLD]\n",
    "\n",
    "    if len(high_confidence_tiles) > 0:\n",
    "        # Use all high confidence tiles\n",
    "        best_tiles = high_confidence_tiles\n",
    "    else:\n",
    "        # Filter tiles with enough matches\n",
    "        good_tiles = [tile for tile in sorted_tiles if tile['num_matches'] >= MIN_MATCHES_THRESHOLD]\n",
    "        \n",
    "        if len(good_tiles) > 0:\n",
    "            # Use up to NUM_BEST_TILES from good tiles\n",
    "            best_tiles = good_tiles[:NUM_BEST_TILES]\n",
    "        else:\n",
    "            # If no tile meets minimum threshold, use at least the best one\n",
    "            best_tiles = [sorted_tiles[0]]\n",
    "            \n",
    "    # Process multiple best tiles and collect predictions\n",
    "    predictions_x = []\n",
    "    predictions_y = []\n",
    "    \n",
    "    for idx, tile_result in enumerate(best_tiles, 1):\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        tile_img = tile_result['tile_img']\n",
    "        num_matches = tile_result['num_matches']\n",
    "        best_rotation_idx = tile_result['rotation_index']\n",
    "        rotated_image = tile_result['rotated_image']\n",
    "        \n",
    "        # Only use tiles with at least some matches\n",
    "        if num_matches > 0:\n",
    "            pred_x, pred_y = compute_pose_prediction(rotated_image, img_path, tile_img, train_pos, \n",
    "                                          zoom_factor=1, map_features=None, plot_gt=False)\n",
    "            \n",
    "            # Adjust predicted position to full map coordinates\n",
    "            full_map_pred_x = pred_x + tile_pos['left']\n",
    "            full_map_pred_y = pred_y + tile_pos['top']\n",
    "            \n",
    "            predictions_x.append(full_map_pred_x)\n",
    "            predictions_y.append(full_map_pred_y)\n",
    "    \n",
    "    # Calculate mean prediction from multiple tiles\n",
    "    if predictions_x:\n",
    "        final_pred_x = sum(predictions_x) / len(predictions_x)\n",
    "        final_pred_y = sum(predictions_y) / len(predictions_y)\n",
    "        num_tiles_used = len(predictions_x)\n",
    "    \n",
    "    # Write prediction to CSV immediately\n",
    "    with open(output_path, 'a') as f:\n",
    "        f.write(f'{img_id},{full_map_pred_x:.1f},{full_map_pred_y:.1f}\\n')\n",
    "\n",
    "print(f\"\\nSaved predictions to {output_path}\")\n",
    "\n",
    "# Quick validation\n",
    "predictions_df = pd.read_csv(output_path)\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(\"Sample predictions:\")\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on ALL train data and evaluate against ground truth\n",
    "TRAIN_IMAGE_PATH = data_path / \"train_data\" / \"train_images\"\n",
    "train_image_paths = sorted(TRAIN_IMAGE_PATH.iterdir())\n",
    "\n",
    "print(f\"Processing {len(train_image_paths)} train images...\")\n",
    "\n",
    "MIN_MATCHES_THRESHOLD = 10  # Minimum matches to consider a tile\n",
    "HIGH_CONFIDENCE_THRESHOLD = 40  # If a tile has this many matches, use only this tile\n",
    "\n",
    "# Prepare CSV file with headers\n",
    "output_path = data_path / 'predictions_lightglue_train.csv'\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write('id,x_pixel,y_pixel,gt_x_pixel,gt_y_pixel,error_distance,num_tiles_used\\n')\n",
    "\n",
    "for i, img_path in enumerate(train_image_paths):\n",
    "    if (i+1) % 11 == 0:\n",
    "        break\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing {i+1}/{len(train_image_paths)}: {img_path.name}\")\n",
    "    \n",
    "    img_id = int(img_path.name.split('.')[0])\n",
    "    \n",
    "    # Get ground truth for this image\n",
    "    gt_x, gt_y, _ = get_ground_truth_positions(train_pos, img_path)\n",
    "    \n",
    "    # Resize drone image using calculated dimensions\n",
    "    image0 = load_image(img_path, resize=(target_height, target_width)).to(device)\n",
    "    \n",
    "    # Store tile results with match counts\n",
    "    tile_results = []\n",
    "    \n",
    "    # Generate all 90-degree rotations of the image (0, 90, 180, 270 degrees)\n",
    "    rotated_images = [image0]\n",
    "    for k in range(1, 4):\n",
    "        rotated_images.append(torch.rot90(image0, k, dims=[1, 2]))\n",
    "    for rot_idx, rotated_image in enumerate(rotated_images):\n",
    "        \n",
    "        # Sliding window approach\n",
    "        windows = []\n",
    "        map_tile_positions = []\n",
    "\n",
    "        for top in range(0, MAP_HEIGHT - window_height + 1, stride_y):\n",
    "            for left in range(0, MAP_WIDTH - window_width + 1, stride_x):\n",
    "                bottom = top + window_height\n",
    "                right = left + window_width\n",
    "                window = image_map[:, top:bottom, left:right]\n",
    "                windows.append(window)\n",
    "                map_tile_positions.append(\n",
    "                    {\"top\": top,\n",
    "                    \"bottom\": bottom,\n",
    "                    \"left\": left,\n",
    "                    \"right\": right}\n",
    "                    )\n",
    "        \n",
    "        for tile_pos, tile_img in zip(map_tile_positions, windows):\n",
    "            # Extract and match to count matches without plotting\n",
    "            m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(rotated_image, tile_img, map_features=None)\n",
    "            num_matches = len(m_kpts0)\n",
    "            \n",
    "            # Store results\n",
    "            tile_results.append({\n",
    "                'tile_pos': tile_pos,\n",
    "                'tile_img': tile_img,\n",
    "                'num_matches': num_matches,\n",
    "                'matches_data': (m_kpts0, m_kpts1, matches01, kpts0, kpts1),\n",
    "                'rotation_index': rot_idx,\n",
    "                'rotated_image': rotated_image\n",
    "            })\n",
    "        \n",
    "    # Sort by number of matches (descending) and apply filtering logic\n",
    "    sorted_tiles = sorted(tile_results, key=lambda x: x['num_matches'], reverse=True)\n",
    "\n",
    "    # Check if any tiles have high confidence\n",
    "    high_confidence_tiles = [tile for tile in sorted_tiles if tile['num_matches'] >= HIGH_CONFIDENCE_THRESHOLD]\n",
    "\n",
    "    if len(high_confidence_tiles) > 0:\n",
    "        # Use all high confidence tiles\n",
    "        best_tiles = high_confidence_tiles\n",
    "    else:\n",
    "        # Filter tiles with enough matches\n",
    "        good_tiles = [tile for tile in sorted_tiles if tile['num_matches'] >= MIN_MATCHES_THRESHOLD]\n",
    "        \n",
    "        if len(good_tiles) > 0:\n",
    "            # Use up to NUM_BEST_TILES from good tiles\n",
    "            best_tiles = good_tiles[:NUM_BEST_TILES]\n",
    "        else:\n",
    "            # If no tile meets minimum threshold, use at least the best one\n",
    "            best_tiles = [sorted_tiles[0]]\n",
    "            \n",
    "    # Process multiple best tiles and collect predictions\n",
    "    predictions_x = []\n",
    "    predictions_y = []\n",
    "    \n",
    "    for idx, tile_result in enumerate(best_tiles, 1):\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        tile_img = tile_result['tile_img']\n",
    "        num_matches = tile_result['num_matches']\n",
    "        best_rotation_idx = tile_result['rotation_index']\n",
    "        rotated_image = tile_result['rotated_image']\n",
    "        \n",
    "        # Only use tiles with at least some matches\n",
    "        if num_matches > 0:\n",
    "            pred_x, pred_y = compute_pose_prediction(rotated_image, img_path, tile_img, train_pos, \n",
    "                                          zoom_factor=1, map_features=None, plot_gt=False)\n",
    "            \n",
    "            # Adjust predicted position to full map coordinates\n",
    "            full_map_pred_x = pred_x + tile_pos['left']\n",
    "            full_map_pred_y = pred_y + tile_pos['top']\n",
    "            \n",
    "            predictions_x.append(full_map_pred_x)\n",
    "            predictions_y.append(full_map_pred_y)\n",
    "    \n",
    "    # Calculate mean prediction from multiple tiles\n",
    "    if predictions_x:\n",
    "        final_pred_x = sum(predictions_x) / len(predictions_x)\n",
    "        final_pred_y = sum(predictions_y) / len(predictions_y)\n",
    "        num_tiles_used = len(predictions_x)\n",
    "    \n",
    "    # Calculate error distance\n",
    "    error_distance = ((final_pred_x - gt_x)**2 + (final_pred_y - gt_y)**2)**0.5\n",
    "    \n",
    "    # Write prediction and evaluation to CSV immediately\n",
    "    with open(output_path, 'a') as f:\n",
    "        f.write(f'{img_id},{final_pred_x:.1f},{final_pred_y:.1f},{gt_x:.1f},{gt_y:.1f},{error_distance:.1f},{num_tiles_used}\\n')\n",
    "\n",
    "print(f\"\\nSaved predictions to {output_path}\")\n",
    "\n",
    "# Quick validation and evaluation\n",
    "predictions_df = pd.read_csv(output_path)\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(\"Sample predictions:\")\n",
    "print(predictions_df.head())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mean_error = predictions_df['error_distance'].mean()\n",
    "median_error = predictions_df['error_distance'].median()\n",
    "std_error = predictions_df['error_distance'].std()\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Mean error: {mean_error:.1f} pixels\")\n",
    "print(f\"Median error: {median_error:.1f} pixels\")\n",
    "print(f\"Std error: {std_error:.1f} pixels\")\n",
    "print(f\"Min error: {predictions_df['error_distance'].min():.1f} pixels\")\n",
    "print(f\"Max error: {predictions_df['error_distance'].max():.1f} pixels\")\n",
    "print(f\"Average tiles used: {predictions_df['num_tiles_used'].mean():.1f}\")\n",
    "\n",
    "# Accuracy at different thresholds\n",
    "thresholds = [50, 100, 200, 500]\n",
    "for threshold in thresholds:\n",
    "    accuracy = (predictions_df['error_distance'] <= threshold).mean() * 100\n",
    "    print(f\"Accuracy within {threshold} pixels: {accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy example\n",
    "The top image shows the matches, while the bottom image shows the point pruning across layers. In this case, LightGlue prunes a few points with occlusions, but is able to stop the context aggregation after 4/9 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image0 = load_image(images / \"DSC_0411.JPG\")\n",
    "image1 = load_image(images / \"DSC_0410.JPG\")\n",
    "\n",
    "feats0 = extractor.extract(image0.to(device))\n",
    "feats1 = extractor.extract(image1.to(device))\n",
    "matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n",
    "feats0, feats1, matches01 = [\n",
    "    rbd(x) for x in [feats0, feats1, matches01]\n",
    "]  # remove batch dimension\n",
    "\n",
    "kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "\n",
    "axes = viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_matches(m_kpts0, m_kpts1, color=\"lime\", lw=0.2)\n",
    "viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers', fs=20)\n",
    "\n",
    "kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficult example\n",
    "For pairs with significant viewpoint- and illumination changes, LightGlue can exclude a lot of points early in the matching process (red points), which significantly reduces the inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image0 = load_image(images / \"sacre_coeur1.jpg\")\n",
    "image1 = load_image(images / \"sacre_coeur2.jpg\")\n",
    "\n",
    "feats0 = extractor.extract(image0.to(device))\n",
    "feats1 = extractor.extract(image1.to(device))\n",
    "matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n",
    "feats0, feats1, matches01 = [\n",
    "    rbd(x) for x in [feats0, feats1, matches01]\n",
    "]  # remove batch dimension\n",
    "\n",
    "kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "\n",
    "axes = viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_matches(m_kpts0, m_kpts1, color=\"lime\", lw=0.2)\n",
    "viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers')\n",
    "\n",
    "kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".se3loc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
