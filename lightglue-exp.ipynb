{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGlue Based Image-Map Matching\n",
    "this notebook is based on the [LightGlue Demo](https://github.com/cvg/LightGlue/blob/main/demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are on colab: this clones the repo and installs the dependencies\n",
    "from pathlib import Path\n",
    "\n",
    "# if Path.cwd().name != \"LightGlue\":\n",
    "#     if not Path(\"LightGlue\").exists():\n",
    "#         !git clone --quiet https://github.com/cvg/LightGlue/\n",
    "#     %cd LightGlue\n",
    "    # !pip install --progress-bar off --quiet -e .\n",
    "\n",
    "from lightglue import LightGlue, SuperPoint, DISK\n",
    "from lightglue.utils import load_image, rbd\n",
    "from lightglue import viz2d\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "images = Path(\"assets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load extractor and matcher module\n",
    "In this example we use SuperPoint features combined with LightGlue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability and set device appropriately\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")  \n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_num_keypoints=2048\n",
    "max_num_keypoints = None\n",
    "extractor = SuperPoint(max_num_keypoints = max_num_keypoints).eval().to(device)  # load the extractor\n",
    "matcher = LightGlue(features=\"superpoint\").eval().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/home/andrii/edth/2/gnss-denied-localization-munich/data\")\n",
    "# Load images and move to device (works for both CUDA and CPU)\n",
    "image_map = load_image(data_path / 'map.png').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training positions\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "train_pos = pd.read_csv(data_path / 'train_data/train_pos.csv')\n",
    "\n",
    "def extract_and_match(image0, image_map, map_features=None):\n",
    "    feats0 = extractor.extract(image0)\n",
    "    if map_features is not None:\n",
    "        feats1 = map_features\n",
    "    else:\n",
    "        feats1 = extractor.extract(image_map)\n",
    "    matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n",
    "    feats0, feats1, matches01 = [\n",
    "        rbd(x) for x in [feats0, feats1, matches01]\n",
    "    ]  # remove batch dimension\n",
    "\n",
    "    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "    # print(f\"__Number of matches: {len(m_kpts0)}\")\n",
    "    return m_kpts0, m_kpts1, matches01, kpts0, kpts1\n",
    "\n",
    "def get_ground_truth_positions(train_pos, img_path):\n",
    "    # Get id from filename\n",
    "    id = int(img_path.name.split('.')[0])\n",
    "    # Get the ground truth positions for the current image\n",
    "    gt_positions = train_pos[train_pos['id'] == id]\n",
    "    gt_x = gt_positions.iloc[0]['x_pixel']\n",
    "    gt_y = gt_positions.iloc[0]['y_pixel']\n",
    "    return gt_x, gt_y, id\n",
    "\n",
    "def get_zoomed_map(image_map, gt_x, gt_y, zoom_factor=5):\n",
    "    # Get map dimensions\n",
    "    map_height, map_width = image_map.shape[1], image_map.shape[2]\n",
    "    \n",
    "    # Calculate zoom window (4x zoom means 1/4 of original size)\n",
    "    window_width = map_width // zoom_factor\n",
    "    window_height = map_height // zoom_factor\n",
    "    \n",
    "    # Calculate crop boundaries centered on ground truth\n",
    "    left = max(0, int(gt_x - window_width // 2))\n",
    "    right = min(map_width, int(gt_x + window_width // 2))\n",
    "    top = max(0, int(gt_y - window_height // 2))\n",
    "    bottom = min(map_height, int(gt_y + window_height // 2))\n",
    "    \n",
    "    # Crop the map\n",
    "    zoomed_map = image_map[:, top:bottom, left:right]\n",
    "\n",
    "    # Adjust ground truth coordinates for the cropped image\n",
    "    adj_gt_x = gt_x - left\n",
    "    adj_gt_y = gt_y - top\n",
    "    \n",
    "    return zoomed_map, adj_gt_x, adj_gt_y\n",
    "\n",
    "def get_map_tile_features(image_map, zoom_factor=5):\n",
    "    \"\"\"\n",
    "    Extract features for map tiles at a specified zoom factor, creating a grid of tiles across the entire map.\n",
    "    \"\"\"\n",
    "    # Get map dimensions\n",
    "    map_height, map_width = image_map.shape[1], image_map.shape[2]\n",
    "    \n",
    "    # Calculate tile dimensions\n",
    "    tile_width = map_width // zoom_factor\n",
    "    tile_height = map_height // zoom_factor\n",
    "    \n",
    "    tile_features = []\n",
    "    tile_positions = []\n",
    "    tiles = []\n",
    "    \n",
    "    # Create grid of tiles\n",
    "    for row in range(zoom_factor):\n",
    "        for col in range(zoom_factor):\n",
    "            # Calculate tile boundaries\n",
    "            left = col * tile_width\n",
    "            right = min((col + 1) * tile_width, map_width)\n",
    "            top = row * tile_height\n",
    "            bottom = min((row + 1) * tile_height, map_height)\n",
    "            \n",
    "            # Extract tile\n",
    "            tile = image_map[:, top:bottom, left:right]\n",
    "            \n",
    "            # Extract features from the tile\n",
    "            feats = extractor.extract(tile)\n",
    "            feats = rbd(feats)  # remove batch dimension\n",
    "            \n",
    "            tile_features.append(feats)\n",
    "            tile_positions.append({\n",
    "                'row': row, \n",
    "                'col': col, \n",
    "                'left': left, \n",
    "                'right': right, \n",
    "                'top': top, \n",
    "                'bottom': bottom\n",
    "            })\n",
    "            tiles.append(tile)\n",
    "    \n",
    "    return tile_features, tile_positions, tiles\n",
    "\n",
    "def plot_matches(image0, img_path, image_map, train_pos, zoom_factor=5, map_features=None, plot_gt=False):\n",
    "    gt_x, gt_y, id = get_ground_truth_positions(train_pos, img_path)\n",
    "    print(f\"__Plotting ground truth for ID: {id}\")\n",
    "    print(f\"__Ground truth position: ({gt_x:.1f}, {gt_y:.1f})\")\n",
    "\n",
    "    # Get zoomed map and adjusted ground truth coordinates\n",
    "    if zoom_factor > 1:\n",
    "        zoomed_map, adj_gt_x, adj_gt_y = get_zoomed_map(image_map, gt_x, gt_y, zoom_factor)\n",
    "        image_map = zoomed_map.to(device)\n",
    "        gt_x, gt_y = adj_gt_x, adj_gt_y\n",
    "\n",
    "    # Extract and match keypoints\n",
    "    m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(image0, image_map, map_features)\n",
    "    # Visualize the images\n",
    "    _ = viz2d.plot_images([image0, image_map], titles=[f\"Image {id}\", f\"Map (Zoom {zoom_factor}x)\"])\n",
    "    # Get the current figure and axes\n",
    "    fig = plt.gcf()\n",
    "    axes = fig.axes\n",
    "    \n",
    "    # Plot red dot on the map (image1) at ground truth position\n",
    "    if plot_gt:\n",
    "        axes[1].plot(gt_x, gt_y, 'ro', markersize=8, markeredgecolor='white', markeredgewidth=1)\n",
    "        axes[1].text(gt_x + 20, gt_y - 20, f'GT ({gt_x:.0f}, {gt_y:.0f})', \n",
    "                    color='red', fontsize=10, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    # Plot matches\n",
    "    viz2d.plot_matches(m_kpts0, m_kpts1, color=\"blue\", lw=0.3)\n",
    "    viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers')\n",
    "    \n",
    "    # Compute center of map matches and plot\n",
    "    if len(m_kpts1) > 0:\n",
    "        center_x = m_kpts1[:, 0].mean().item()\n",
    "        center_y = m_kpts1[:, 1].mean().item()\n",
    "        axes[1].plot(center_x, center_y, 'go', markersize=10, markeredgecolor='white', markeredgewidth=2)\n",
    "        axes[1].text(center_x + 20, center_y + 20, f'Center ({center_x:.0f}, {center_y:.0f})', \n",
    "                    color='green', fontsize=10, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return center_x, center_y\n",
    "\n",
    "    # # Plot pruned keypoints\n",
    "    # kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "    # viz2d.plot_images([image0, image_map])\n",
    "    # viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=6)\n",
    "\n",
    "\n",
    "def compute_pose_prediction(image0, img_path, image_map, train_pos, zoom_factor=5, map_features=None, plot_gt=False):\n",
    "    # Extract and match keypoints\n",
    "    m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(image0, image_map, map_features)\n",
    "    \n",
    "    # Compute center of map matches and plot\n",
    "    if len(m_kpts1) > 0:\n",
    "        center_x = m_kpts1[:, 0].mean().item()\n",
    "        center_y = m_kpts1[:, 1].mean().item()\n",
    "\n",
    "    return center_x, center_y\n",
    "\n",
    "def compute_pose_prediction_with_transform(image0, img_path, image_map, train_pos, zoom_factor=5, map_features=None, plot_gt=False):\n",
    "    \"\"\"\n",
    "    Compute pose prediction using geometric transformation between matched keypoints.\n",
    "    Estimates translation, rotation, and scale, then applies to image center.\n",
    "    \"\"\"\n",
    "    # Extract and match keypoints\n",
    "    m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(image0, image_map, map_features)\n",
    "    \n",
    "    if len(m_kpts1) < 4:  # Need at least 4 points for robust transform estimation\n",
    "        # Fallback to centroid method\n",
    "        if len(m_kpts1) > 0:\n",
    "            center_x = m_kpts1[:, 0].mean().item()\n",
    "            center_y = m_kpts1[:, 1].mean().item()\n",
    "            return center_x, center_y, None\n",
    "        else:\n",
    "            # No matches - return map center\n",
    "            map_height, map_width = image_map.shape[1], image_map.shape[2]\n",
    "            return map_width // 2, map_height // 2, None\n",
    "    \n",
    "    # Convert to numpy arrays for OpenCV\n",
    "    pts_drone = m_kpts0.cpu().numpy().astype(np.float32)\n",
    "    pts_map = m_kpts1.cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # Method 1: Estimate similarity transform (translation + rotation + uniform scale)\n",
    "    try:\n",
    "        # Use RANSAC to find robust similarity transform\n",
    "        transform_matrix, inliers = cv2.estimateAffinePartial2D(\n",
    "            pts_drone, pts_map, \n",
    "            method=cv2.RANSAC,\n",
    "            ransacReprojThreshold=10.0,  # Adjust based on expected accuracy\n",
    "            maxIters=2000,\n",
    "            confidence=0.99\n",
    "        )\n",
    "        \n",
    "        if transform_matrix is not None and inliers is not None:\n",
    "            # Get image center in drone coordinates\n",
    "            drone_height, drone_width = image0.shape[1], image0.shape[2]\n",
    "            drone_center = np.array([[drone_width // 2, drone_height // 2]], dtype=np.float32)\n",
    "            \n",
    "            # Transform drone center to map coordinates\n",
    "            map_center = cv2.transform(drone_center.reshape(1, 1, 2), transform_matrix)\n",
    "            center_x, center_y = map_center[0, 0, 0], map_center[0, 0, 1]\n",
    "            \n",
    "            # Extract transformation parameters\n",
    "            transform_info = {\n",
    "                'matrix': transform_matrix,\n",
    "                'translation': [transform_matrix[0, 2], transform_matrix[1, 2]],\n",
    "                'scale': np.sqrt(transform_matrix[0, 0]**2 + transform_matrix[0, 1]**2),\n",
    "                'rotation_deg': np.degrees(np.arctan2(transform_matrix[1, 0], transform_matrix[0, 0])),\n",
    "                'inliers_count': np.sum(inliers),\n",
    "                'inlier_ratio': np.sum(inliers) / len(inliers)\n",
    "            }\n",
    "            \n",
    "            return center_x, center_y, transform_info\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Similarity transform failed: {e}\")\n",
    "    \n",
    "    # Method 2: Fallback to homography (more flexible but can be less stable)\n",
    "    try:\n",
    "        homography, mask = cv2.findHomography(\n",
    "            pts_drone, pts_map,\n",
    "            cv2.RANSAC,\n",
    "            ransacReprojThreshold=10.0\n",
    "        )\n",
    "        \n",
    "        if homography is not None:\n",
    "            # Transform drone center using homography\n",
    "            drone_height, drone_width = image0.shape[1], image0.shape[2]\n",
    "            drone_center = np.array([[[drone_width // 2, drone_height // 2]]], dtype=np.float32)\n",
    "            \n",
    "            map_center = cv2.perspectiveTransform(drone_center, homography)\n",
    "            center_x, center_y = map_center[0, 0, 0], map_center[0, 0, 1]\n",
    "            \n",
    "            transform_info = {\n",
    "                'matrix': homography,\n",
    "                'method': 'homography',\n",
    "                'inliers_count': np.sum(mask),\n",
    "                'inlier_ratio': np.sum(mask) / len(mask) if mask is not None else 0\n",
    "            }\n",
    "            \n",
    "            return center_x, center_y, transform_info\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Homography failed: {e}\")\n",
    "    \n",
    "    # Method 3: Final fallback to weighted centroid\n",
    "    if len(m_kpts1) > 0:\n",
    "        # Use match confidence if available\n",
    "        if \"matching_scores0\" in matches01:\n",
    "            matches = matches01[\"matches\"]\n",
    "            scores = matches01[\"matching_scores0\"][matches[..., 0]]\n",
    "            weights = torch.softmax(scores, dim=0).cpu().numpy()\n",
    "            \n",
    "            center_x = np.average(m_kpts1[:, 0].cpu().numpy(), weights=weights)\n",
    "            center_y = np.average(m_kpts1[:, 1].cpu().numpy(), weights=weights)\n",
    "        else:\n",
    "            center_x = m_kpts1[:, 0].mean().item()\n",
    "            center_y = m_kpts1[:, 1].mean().item()\n",
    "        \n",
    "        transform_info = {\n",
    "            'method': 'weighted_centroid',\n",
    "            'num_matches': len(m_kpts1)\n",
    "        }\n",
    "        \n",
    "        return center_x, center_y, transform_info\n",
    "    \n",
    "    # Ultimate fallback - map center\n",
    "    map_height, map_width = image_map.shape[1], image_map.shape[2]\n",
    "    return map_width // 2, map_height // 2, {'method': 'map_center'}\n",
    "\n",
    "\n",
    "def compute_pose_prediction_robust(image0, img_path, image_map, train_pos, zoom_factor=5, map_features=None, plot_gt=False):\n",
    "    \"\"\"\n",
    "    Enhanced version with multiple geometric estimation methods and quality assessment.\n",
    "    \"\"\"\n",
    "    # Extract and match keypoints\n",
    "    m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(image0, image_map, map_features)\n",
    "    \n",
    "    if len(m_kpts1) < 3:\n",
    "        # Not enough matches for geometric estimation\n",
    "        if len(m_kpts1) > 0:\n",
    "            center_x = m_kpts1[:, 0].mean().item()\n",
    "            center_y = m_kpts1[:, 1].mean().item()\n",
    "        else:\n",
    "            map_height, map_width = image_map.shape[1], image_map.shape[2]\n",
    "            center_x, center_y = map_width // 2, map_height // 2\n",
    "        \n",
    "        return center_x, center_y, {'method': 'simple_centroid', 'num_matches': len(m_kpts1)}\n",
    "    \n",
    "    # Convert to numpy\n",
    "    pts_drone = m_kpts0.cpu().numpy().astype(np.float32)\n",
    "    pts_map = m_kpts1.cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # Get drone image center\n",
    "    drone_height, drone_width = image0.shape[1], image0.shape[2]\n",
    "    drone_center = np.array([drone_width // 2, drone_height // 2], dtype=np.float32)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Method 1: Similarity Transform (best for drone imagery)\n",
    "    if len(m_kpts1) >= 2:\n",
    "        try:\n",
    "            # Estimate similarity transform with RANSAC\n",
    "            sim_matrix, inliers = cv2.estimateAffinePartial2D(\n",
    "                pts_drone, pts_map,\n",
    "                method=cv2.RANSAC,\n",
    "                ransacReprojThreshold=15.0,\n",
    "                maxIters=1000,\n",
    "                confidence=0.95\n",
    "            )\n",
    "            \n",
    "            if sim_matrix is not None and inliers is not None:\n",
    "                # Transform drone center\n",
    "                transformed_center = cv2.transform(\n",
    "                    drone_center.reshape(1, 1, 2), sim_matrix\n",
    "                )[0, 0]\n",
    "                \n",
    "                inlier_count = np.sum(inliers)\n",
    "                inlier_ratio = inlier_count / len(inliers)\n",
    "                \n",
    "                # Calculate transform parameters\n",
    "                tx, ty = sim_matrix[0, 2], sim_matrix[1, 2]\n",
    "                scale = np.sqrt(sim_matrix[0, 0]**2 + sim_matrix[0, 1]**2)\n",
    "                rotation = np.degrees(np.arctan2(sim_matrix[1, 0], sim_matrix[0, 0]))\n",
    "                \n",
    "                quality_score = inlier_ratio * min(inlier_count / 10.0, 1.0)  # Prefer more inliers\n",
    "                \n",
    "                results.append({\n",
    "                    'center': transformed_center,\n",
    "                    'method': 'similarity_transform',\n",
    "                    'quality': quality_score,\n",
    "                    'inliers': inlier_count,\n",
    "                    'inlier_ratio': inlier_ratio,\n",
    "                    'translation': [tx, ty],\n",
    "                    'scale': scale,\n",
    "                    'rotation_deg': rotation,\n",
    "                    'matrix': sim_matrix\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Method 2: Least Squares Rigid Transform (translation + rotation only)\n",
    "    if len(m_kpts1) >= 2:\n",
    "        try:\n",
    "            # Compute centroids\n",
    "            centroid_drone = np.mean(pts_drone, axis=0)\n",
    "            centroid_map = np.mean(pts_map, axis=0)\n",
    "            \n",
    "            # Center the points\n",
    "            pts_drone_centered = pts_drone - centroid_drone\n",
    "            pts_map_centered = pts_map - centroid_map\n",
    "            \n",
    "            # Compute rotation using SVD\n",
    "            H = pts_drone_centered.T @ pts_map_centered\n",
    "            U, S, Vt = np.linalg.svd(H)\n",
    "            R = Vt.T @ U.T\n",
    "            \n",
    "            # Ensure proper rotation (det(R) = 1)\n",
    "            if np.linalg.det(R) < 0:\n",
    "                Vt[-1, :] *= -1\n",
    "                R = Vt.T @ U.T\n",
    "            \n",
    "            # Compute translation\n",
    "            t = centroid_map - R @ centroid_drone\n",
    "            \n",
    "            # Transform drone center\n",
    "            transformed_center = R @ drone_center + t\n",
    "            \n",
    "            # Compute residual error\n",
    "            transformed_pts = (R @ pts_drone.T).T + t\n",
    "            errors = np.linalg.norm(transformed_pts - pts_map, axis=1)\n",
    "            mean_error = np.mean(errors)\n",
    "            \n",
    "            quality_score = max(0, 1.0 - mean_error / 50.0)  # Normalize error\n",
    "            \n",
    "            results.append({\n",
    "                'center': transformed_center,\n",
    "                'method': 'rigid_transform',\n",
    "                'quality': quality_score,\n",
    "                'mean_error': mean_error,\n",
    "                'rotation_matrix': R,\n",
    "                'translation': t,\n",
    "                'rotation_deg': np.degrees(np.arctan2(R[1, 0], R[0, 0]))\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Method 3: Weighted centroid (always available)\n",
    "    if \"matching_scores0\" in matches01:\n",
    "        matches = matches01[\"matches\"]\n",
    "        scores = matches01[\"matching_scores0\"][matches[..., 0]]\n",
    "        weights = torch.softmax(scores, dim=0).cpu().numpy()\n",
    "        \n",
    "        center_x = np.average(m_kpts1[:, 0].cpu().numpy(), weights=weights)\n",
    "        center_y = np.average(m_kpts1[:, 1].cpu().numpy(), weights=weights)\n",
    "    else:\n",
    "        center_x = m_kpts1[:, 0].mean().item()\n",
    "        center_y = m_kpts1[:, 1].mean().item()\n",
    "    \n",
    "    results.append({\n",
    "        'center': np.array([center_x, center_y]),\n",
    "        'method': 'weighted_centroid',\n",
    "        'quality': 0.5,  # Medium confidence\n",
    "        'num_matches': len(m_kpts1)\n",
    "    })\n",
    "    \n",
    "    # Select best result based on quality score\n",
    "    best_result = max(results, key=lambda x: x['quality'])\n",
    "    \n",
    "    return best_result['center'][0], best_result['center'][1], best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGE_PATH = data_path / \"test_data\" / \"test_images\"\n",
    "TRAIN_IMAGE_PATH = data_path / \"train_data\" / \"train_images\"\n",
    "\n",
    "image_paths = sorted(TRAIN_IMAGE_PATH.iterdir())\n",
    "\n",
    "# Configuration parameters\n",
    "DRONE_SCALE_FACTOR = 8  # Scale factor for drone image resizing\n",
    "MAP_ZOOM_FACTOR = int(DRONE_SCALE_FACTOR * 0.625)     # Map tiling factor\n",
    "STRIDE_FACTOR = 2  # 2 = 50% overlap; 1 = stride equals window size\n",
    "\n",
    "# Known resolutions\n",
    "DRONE_WIDTH = 8004\n",
    "DRONE_HEIGHT = 6001\n",
    "MAP_WIDTH = 5000\n",
    "MAP_HEIGHT = 2500\n",
    "\n",
    "NUM_BEST_TILES = 5\n",
    "\n",
    "# Calculate target resize dimensions\n",
    "target_width = DRONE_WIDTH // DRONE_SCALE_FACTOR   # 8004 // 6 = 1334\n",
    "target_height = DRONE_HEIGHT // DRONE_SCALE_FACTOR # 6001 // 6 = 1000\n",
    "\n",
    "window_height, window_width = MAP_HEIGHT // MAP_ZOOM_FACTOR, MAP_WIDTH // MAP_ZOOM_FACTOR\n",
    "stride_y, stride_x = window_height // STRIDE_FACTOR, window_width // STRIDE_FACTOR\n",
    "\n",
    "print(f\"Drone image scaling: {DRONE_WIDTH}x{DRONE_HEIGHT} -> {target_width}x{target_height} (factor: {DRONE_SCALE_FACTOR})\")\n",
    "print(f\"Map dimensions: {MAP_WIDTH}x{MAP_HEIGHT} -> Tile size: {window_width}x{window_height} (zoom factor: {MAP_ZOOM_FACTOR})\")\n",
    "print(f\"Stride: {stride_x}x{stride_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for x images in train_data with a step of 50\n",
    "for i in range(0, len(image_paths), 70):\n",
    "    img_path = image_paths[i]\n",
    "    print(f\"Processing image: {img_path.name}...\")\n",
    "    \n",
    "    # Resize drone image using calculated dimensions\n",
    "    image0 = load_image(img_path, resize=(target_height, target_width)).to(device)\n",
    "    \n",
    "    # Store tile results with match counts\n",
    "    tile_results = []\n",
    "    \n",
    "    # Generate all 90-degree rotations of the image (0, 90, 180, 270 degrees)\n",
    "    rotated_images = [image0]\n",
    "    for k in range(1, 4):\n",
    "        rotated_images.append(torch.rot90(image0, k, dims=[1, 2]))\n",
    "    for rot_idx, rotated_image in enumerate(rotated_images):\n",
    "\n",
    "        windows = []\n",
    "        map_tile_positions = []\n",
    "\n",
    "        for top in range(0, MAP_HEIGHT - window_height + 1, stride_y):\n",
    "            for left in range(0, MAP_WIDTH - window_width + 1, stride_x):\n",
    "                bottom = top + window_height\n",
    "                right = left + window_width\n",
    "                window = image_map[:, top:bottom, left:right]\n",
    "                windows.append(window)\n",
    "                map_tile_positions.append(\n",
    "                    {\"top\": top,\n",
    "                    \"bottom\": bottom,\n",
    "                    \"left\": left,\n",
    "                    \"right\": right}\n",
    "                    )\n",
    "        \n",
    "        for tile_pos, tile_img in zip(map_tile_positions, windows):\n",
    "            # Extract and match to count matches without plotting\n",
    "            m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(rotated_image, tile_img, map_features=None)\n",
    "            num_matches = len(m_kpts0)\n",
    "            \n",
    "            # Store results\n",
    "            tile_results.append({\n",
    "                'tile_pos': tile_pos,\n",
    "                'tile_img': tile_img,\n",
    "                'num_matches': num_matches,\n",
    "                'matches_data': (m_kpts0, m_kpts1, matches01, kpts0, kpts1),\n",
    "                'rotation_index': rot_idx,\n",
    "                'rotated_image': rotated_image\n",
    "            })\n",
    "            \n",
    "            # print(f\"__Tile [{tile_pos[\"left\"]}, {tile_pos[\"top\"]}] has {num_matches} matches\")\n",
    "        \n",
    "    # Sort by number of matches (descending) and take top 3\n",
    "    best_tiles = sorted(tile_results, key=lambda x: x['num_matches'], reverse=True)[:NUM_BEST_TILES]\n",
    "    \n",
    "    print(f\"\\n=== Top 3 tiles for image {img_path.name} ===\")\n",
    "    for idx, tile_result in enumerate(best_tiles, 1):\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        num_matches = tile_result['num_matches']\n",
    "        print(f\"{idx}. Tile {tile_pos['left']}, {tile_pos['top']}: {num_matches} matches\")\n",
    "    \n",
    "    # Plot only the best 3 tiles\n",
    "    for idx, tile_result in enumerate(best_tiles, 1):\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        tile_img = tile_result['tile_img']\n",
    "        num_matches = tile_result['num_matches']\n",
    "        best_rotation_idx = tile_result['rotation_index']\n",
    "        rotated_image = tile_result['rotated_image']\n",
    "        \n",
    "        print(f\"\\n--- Plotting #{idx} best tile (image rot= {best_rotation_idx *90}) at: [{tile_pos['left']}, {tile_pos['top']}] with {num_matches} matches ---\")\n",
    "        pred_x, pred_y = plot_matches(rotated_image, img_path, tile_img, train_pos, \n",
    "                                      zoom_factor=1, map_features=None, plot_gt=False)\n",
    "        \n",
    "        # Plot whole map with best tile highlighted, predicted position, and ground truth\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.imshow(image_map.cpu().numpy().transpose(1, 2, 0))\n",
    "        \n",
    "        # Get ground truth position for the current image\n",
    "        gt_x, gt_y, id = get_ground_truth_positions(train_pos, img_path)\n",
    "        \n",
    "        # Highlight the best tile with a rectangle\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        rect = plt.Rectangle((tile_pos['left'], tile_pos['top']), \n",
    "                   tile_pos['right'] - tile_pos['left'], \n",
    "                   tile_pos['bottom'] - tile_pos['top'],\n",
    "                   linewidth=1, edgecolor='blue', facecolor='none', alpha=0.8)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Adjust predicted position to full map coordinates\n",
    "        full_map_pred_x = pred_x + tile_pos['left']\n",
    "        full_map_pred_y = pred_y + tile_pos['top']\n",
    "        \n",
    "        # Plot ground truth position (red dot)\n",
    "        ax.plot(gt_x, gt_y, 'ro', markersize=10, markeredgecolor='white', markeredgewidth=2)\n",
    "        ax.text(gt_x + 50, gt_y - 50, f'GT ({gt_x:.0f}, {gt_y:.0f})', \n",
    "            color='red', fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Plot predicted position (green dot)\n",
    "        ax.plot(full_map_pred_x, full_map_pred_y, 'go', markersize=10, markeredgecolor='white', markeredgewidth=2)\n",
    "        ax.text(full_map_pred_x + 50, full_map_pred_y + 50, f'Pred ({full_map_pred_x:.0f}, {full_map_pred_y:.0f})', \n",
    "            color='green', fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Add tile information\n",
    "        ax.text(tile_pos['left'] + 10, tile_pos['top'] + 30, \n",
    "            f'Best Tile \\n{num_matches} matches', \n",
    "            color='blue', fontsize=10, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        ax.set_title(f'Full Map - Image {id} - Tile Ranking #{idx}')\n",
    "        ax.set_xlabel('X coordinate')\n",
    "        ax.set_ylabel('Y coordinate')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "# Run on ALL test data and save to CSV\n",
    "TEST_IMAGE_PATH = data_path / \"test_data\" / \"test_images\"\n",
    "test_image_paths = sorted(TEST_IMAGE_PATH.iterdir())\n",
    "MIN_MATCHES_THRESHOLD = 10  # Minimum matches to consider a tile\n",
    "HIGH_CONFIDENCE_THRESHOLD = 40  # If a tile has this many matches, use only this tile\n",
    "NUM_BEST_TILES = 1\n",
    "print(f\"Processing {len(test_image_paths)} test images...\")\n",
    "\n",
    "# Prepare CSV file with headers\n",
    "output_path = data_path / 'predictions_lightglue_test.csv'\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write('id,x_pixel,y_pixel\\n')\n",
    "\n",
    "for i, img_path in enumerate(test_image_paths):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing {i+1}/{len(test_image_paths)}: {img_path.name}\")\n",
    "    \n",
    "    img_id = int(img_path.name.split('.')[0])\n",
    "    \n",
    "    # Resize drone image using calculated dimensions\n",
    "    image0 = load_image(img_path, resize=(target_height, target_width)).to(device)\n",
    "    \n",
    "    # Store tile results with match counts\n",
    "    tile_results = []\n",
    "    \n",
    "    # Generate all 90-degree rotations of the image (0, 90, 180, 270 degrees)\n",
    "    rotated_images = []\n",
    "    for angle in [0, 45, 90, 135, 180, 225, 270, 315]:\n",
    "        if angle == 0:\n",
    "            rotated_images.append(image0)\n",
    "        else:\n",
    "            rotated_img = TF.rotate(image0.permute(1, 2, 0), angle, fill=0).permute(2, 0, 1)\n",
    "            rotated_images.append(rotated_img)\n",
    "    for rot_idx, rotated_image in enumerate(rotated_images):\n",
    "    \n",
    "        # Sliding window approach\n",
    "        windows = []\n",
    "        map_tile_positions = []\n",
    "\n",
    "        for top in range(0, MAP_HEIGHT - window_height + 1, stride_y):\n",
    "            for left in range(0, MAP_WIDTH - window_width + 1, stride_x):\n",
    "                bottom = top + window_height\n",
    "                right = left + window_width\n",
    "                window = image_map[:, top:bottom, left:right]\n",
    "                windows.append(window)\n",
    "                map_tile_positions.append(\n",
    "                    {\"top\": top,\n",
    "                    \"bottom\": bottom,\n",
    "                    \"left\": left,\n",
    "                    \"right\": right}\n",
    "                    )\n",
    "        \n",
    "        for tile_pos, tile_img in zip(map_tile_positions, windows):\n",
    "            # Extract and match to count matches without plotting\n",
    "            m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(rotated_image, tile_img, map_features=None)\n",
    "            num_matches = len(m_kpts0)\n",
    "            \n",
    "            # Store results\n",
    "            tile_results.append({\n",
    "                'tile_pos': tile_pos,\n",
    "                'tile_img': tile_img,\n",
    "                'num_matches': num_matches,\n",
    "                'matches_data': (m_kpts0, m_kpts1, matches01, kpts0, kpts1),\n",
    "                'rotation_index': rot_idx,\n",
    "                'rotated_image': rotated_image\n",
    "            })\n",
    "        \n",
    "     # Sort by number of matches (descending) and apply filtering logic\n",
    "    sorted_tiles = sorted(tile_results, key=lambda x: x['num_matches'], reverse=True)\n",
    "\n",
    "    # Check if any tiles have high confidence\n",
    "    high_confidence_tiles = [tile for tile in sorted_tiles if tile['num_matches'] >= HIGH_CONFIDENCE_THRESHOLD]\n",
    "\n",
    "    if len(high_confidence_tiles) > 0:\n",
    "        # Use all high confidence tiles\n",
    "        best_tiles = high_confidence_tiles\n",
    "    else:\n",
    "        # Filter tiles with enough matches\n",
    "        good_tiles = [tile for tile in sorted_tiles if tile['num_matches'] >= MIN_MATCHES_THRESHOLD]\n",
    "        \n",
    "        if len(good_tiles) > 0:\n",
    "            # Use up to NUM_BEST_TILES from good tiles\n",
    "            best_tiles = good_tiles[:NUM_BEST_TILES]\n",
    "        else:\n",
    "            # If no tile meets minimum threshold, use at least the best one\n",
    "            best_tiles = [sorted_tiles[0]]\n",
    "            \n",
    "    # Process multiple best tiles and collect predictions\n",
    "    predictions_x = []\n",
    "    predictions_y = []\n",
    "    \n",
    "    for idx, tile_result in enumerate(best_tiles, 1):\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        tile_img = tile_result['tile_img']\n",
    "        num_matches = tile_result['num_matches']\n",
    "        best_rotation_idx = tile_result['rotation_index']\n",
    "        rotated_image = tile_result['rotated_image']\n",
    "        \n",
    "        # Only use tiles with at least some matches\n",
    "        if num_matches > 0:\n",
    "            pred_x, pred_y = compute_pose_prediction(rotated_image, img_path, tile_img, train_pos, \n",
    "                                          zoom_factor=1, map_features=None, plot_gt=False)\n",
    "            \n",
    "            # Adjust predicted position to full map coordinates\n",
    "            full_map_pred_x = pred_x + tile_pos['left']\n",
    "            full_map_pred_y = pred_y + tile_pos['top']\n",
    "            \n",
    "            predictions_x.append(full_map_pred_x)\n",
    "            predictions_y.append(full_map_pred_y)\n",
    "    \n",
    "    # Calculate mean prediction from multiple tiles\n",
    "    if predictions_x:\n",
    "        final_pred_x = sum(predictions_x) / len(predictions_x)\n",
    "        final_pred_y = sum(predictions_y) / len(predictions_y)\n",
    "        num_tiles_used = len(predictions_x)\n",
    "    \n",
    "    # Write prediction to CSV immediately\n",
    "    with open(output_path, 'a') as f:\n",
    "        f.write(f'{img_id},{full_map_pred_x:.1f},{full_map_pred_y:.1f}\\n')\n",
    "\n",
    "print(f\"\\nSaved predictions to {output_path}\")\n",
    "\n",
    "# Quick validation\n",
    "predictions_df = pd.read_csv(output_path)\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(\"Sample predictions:\")\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the finetuned LightGlue matcher weights\n",
    "finetuned_weights_path = 'lightglue_finetuned.pth'\n",
    "matcher = LightGlue(features=\"superpoint\").eval().to(device)\n",
    "matcher.load_state_dict(torch.load(finetuned_weights_path, map_location=device))\n",
    "matcher.eval()\n",
    "print(\"Finetuned LightGlue matcher loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Run on ALL train data and evaluate against ground truth\n",
    "TRAIN_IMAGE_PATH = data_path / \"train_data\" / \"train_images\"\n",
    "train_image_paths = sorted(TRAIN_IMAGE_PATH.iterdir())\n",
    "\n",
    "print(f\"Processing {len(train_image_paths)} train images...\")\n",
    "\n",
    "MIN_MATCHES_THRESHOLD = 10  # Minimum matches to consider a tile\n",
    "HIGH_CONFIDENCE_THRESHOLD = 40  # If a tile has this many matches, use only this tile\n",
    "NUM_BEST_TILES = 5\n",
    "\n",
    "# Prepare CSV file with headers\n",
    "output_path = data_path / 'predictions_lightglue_train.csv'\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write('id,x_pixel,y_pixel,gt_x_pixel,gt_y_pixel,error_distance,num_tiles_used\\n')\n",
    "\n",
    "for i, img_path in tqdm(enumerate(train_image_paths)):\n",
    "    if (i+1) % 11 == 0:\n",
    "        break\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing {i+1}/{len(train_image_paths)}: {img_path.name}\")\n",
    "    \n",
    "    img_id = int(img_path.name.split('.')[0])\n",
    "    \n",
    "    # Get ground truth for this image\n",
    "    gt_x, gt_y, _ = get_ground_truth_positions(train_pos, img_path)\n",
    "    \n",
    "    # Resize drone image using calculated dimensions\n",
    "    image0 = load_image(img_path, resize=(target_height, target_width)).to(device)\n",
    "    \n",
    "    # Store tile results with match counts\n",
    "    tile_results = []\n",
    "    \n",
    "    # Generate all 90-degree rotations of the image (0, 90, 180, 270 degrees)\n",
    "    rotated_images = [image0]\n",
    "    for k in range(1, 4):\n",
    "        rotated_images.append(torch.rot90(image0, k, dims=[1, 2]))\n",
    "    for rot_idx, rotated_image in enumerate(rotated_images):\n",
    "        \n",
    "        # Sliding window approach\n",
    "        windows = []\n",
    "        map_tile_positions = []\n",
    "\n",
    "        for top in range(0, MAP_HEIGHT - window_height + 1, stride_y):\n",
    "            for left in range(0, MAP_WIDTH - window_width + 1, stride_x):\n",
    "                bottom = top + window_height\n",
    "                right = left + window_width\n",
    "                window = image_map[:, top:bottom, left:right]\n",
    "                windows.append(window)\n",
    "                map_tile_positions.append(\n",
    "                    {\"top\": top,\n",
    "                    \"bottom\": bottom,\n",
    "                    \"left\": left,\n",
    "                    \"right\": right}\n",
    "                    )\n",
    "        \n",
    "        for tile_pos, tile_img in zip(map_tile_positions, windows):\n",
    "            # Extract and match to count matches without plotting\n",
    "            m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(rotated_image, tile_img, map_features=None)\n",
    "            num_matches = len(m_kpts0)\n",
    "            \n",
    "            # Store results\n",
    "            tile_results.append({\n",
    "                'tile_pos': tile_pos,\n",
    "                'tile_img': tile_img,\n",
    "                'num_matches': num_matches,\n",
    "                'matches_data': (m_kpts0, m_kpts1, matches01, kpts0, kpts1),\n",
    "                'rotation_index': rot_idx,\n",
    "                'rotated_image': rotated_image\n",
    "            })\n",
    "        \n",
    "    # Sort by number of matches (descending) and apply filtering logic\n",
    "    sorted_tiles = sorted(tile_results, key=lambda x: x['num_matches'], reverse=True)\n",
    "\n",
    "    # Check if any tiles have high confidence\n",
    "    high_confidence_tiles = [tile for tile in sorted_tiles if tile['num_matches'] >= HIGH_CONFIDENCE_THRESHOLD]\n",
    "\n",
    "    if len(high_confidence_tiles) > 0:\n",
    "        # Use all high confidence tiles\n",
    "        best_tiles = high_confidence_tiles\n",
    "    else:\n",
    "        # Filter tiles with enough matches\n",
    "        good_tiles = [tile for tile in sorted_tiles if tile['num_matches'] >= MIN_MATCHES_THRESHOLD]\n",
    "        \n",
    "        if len(good_tiles) > 0:\n",
    "            # Use up to NUM_BEST_TILES from good tiles\n",
    "            best_tiles = good_tiles[:NUM_BEST_TILES]\n",
    "        else:\n",
    "            # If no tile meets minimum threshold, use at least the best one\n",
    "            best_tiles = [sorted_tiles[0]]\n",
    "            \n",
    "    # Process multiple best tiles and collect predictions\n",
    "    predictions_x = []\n",
    "    predictions_y = []\n",
    "    \n",
    "    for idx, tile_result in enumerate(best_tiles, 1):\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        tile_img = tile_result['tile_img']\n",
    "        num_matches = tile_result['num_matches']\n",
    "        best_rotation_idx = tile_result['rotation_index']\n",
    "        rotated_image = tile_result['rotated_image']\n",
    "        \n",
    "        # Only use tiles with at least some matches\n",
    "        if num_matches > 0:\n",
    "            pred_x, pred_y = compute_pose_prediction(rotated_image, img_path, tile_img, train_pos, \n",
    "                                          zoom_factor=1, map_features=None, plot_gt=False)\n",
    "            # pred_x, pred_y, _ = compute_pose_prediction_with_transform(rotated_image, img_path, tile_img, train_pos, \n",
    "            #                               zoom_factor=1, map_features=None, plot_gt=False)\n",
    "            \n",
    "            # Adjust predicted position to full map coordinates\n",
    "            full_map_pred_x = pred_x + tile_pos['left']\n",
    "            full_map_pred_y = pred_y + tile_pos['top']\n",
    "            \n",
    "            predictions_x.append(full_map_pred_x)\n",
    "            predictions_y.append(full_map_pred_y)\n",
    "    \n",
    "    # Calculate mean prediction from multiple tiles\n",
    "    if predictions_x:\n",
    "        final_pred_x = sum(predictions_x) / len(predictions_x)\n",
    "        final_pred_y = sum(predictions_y) / len(predictions_y)\n",
    "        num_tiles_used = len(predictions_x)\n",
    "    \n",
    "    # Calculate error distance\n",
    "    error_distance = ((final_pred_x - gt_x)**2 + (final_pred_y - gt_y)**2)**0.5\n",
    "    \n",
    "    # Write prediction and evaluation to CSV immediately\n",
    "    with open(output_path, 'a') as f:\n",
    "        f.write(f'{img_id},{final_pred_x:.1f},{final_pred_y:.1f},{gt_x:.1f},{gt_y:.1f},{error_distance:.1f},{num_tiles_used}\\n')\n",
    "\n",
    "print(f\"\\nSaved predictions to {output_path}\")\n",
    "\n",
    "# Quick validation and evaluation\n",
    "predictions_df = pd.read_csv(output_path)\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(\"Sample predictions:\")\n",
    "print(predictions_df.head())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mean_error = predictions_df['error_distance'].mean()\n",
    "median_error = predictions_df['error_distance'].median()\n",
    "std_error = predictions_df['error_distance'].std()\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Mean error: {mean_error:.1f} pixels\")\n",
    "print(f\"Median error: {median_error:.1f} pixels\")\n",
    "print(f\"Std error: {std_error:.1f} pixels\")\n",
    "print(f\"Min error: {predictions_df['error_distance'].min():.1f} pixels\")\n",
    "print(f\"Max error: {predictions_df['error_distance'].max():.1f} pixels\")\n",
    "print(f\"Average tiles used: {predictions_df['num_tiles_used'].mean():.1f}\")\n",
    "\n",
    "# Accuracy at different thresholds\n",
    "thresholds = [25, 125, 500]\n",
    "accuracies = []\n",
    "for threshold in thresholds:\n",
    "    accuracy = (predictions_df['error_distance'] <= threshold).mean() * 100\n",
    "    print(f\"Accuracy within {threshold} pixels: {accuracy:.1f}%\")\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(f\"Score: {sum(accuracies)/len(accuracies):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DroneMapDataset(Dataset):\n",
    "    def __init__(self, image_paths, train_pos, data_path, target_height, target_width, map_zoom_factor, map_height, map_width, device):\n",
    "        self.image_paths = image_paths\n",
    "        self.train_pos = train_pos\n",
    "        self.data_path = data_path\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "        self.map_zoom_factor = map_zoom_factor\n",
    "        self.map_height = map_height\n",
    "        self.map_width = map_width\n",
    "        self.device = device\n",
    "        \n",
    "        # Load map once\n",
    "        self.image_map = load_image(data_path / 'map.png').to(device)\n",
    "        \n",
    "        # Pre-compute map tiles\n",
    "        self.map_tiles = []\n",
    "        self.tile_positions = []\n",
    "        \n",
    "        window_height = map_height // map_zoom_factor\n",
    "        window_width = map_width // map_zoom_factor\n",
    "        stride_y = window_height // 2  # 50% overlap\n",
    "        stride_x = window_width // 2\n",
    "        \n",
    "        for top in range(0, map_height - window_height + 1, stride_y):\n",
    "            for left in range(0, map_width - window_width + 1, stride_x):\n",
    "                bottom = top + window_height\n",
    "                right = left + window_width\n",
    "                \n",
    "                tile = self.image_map[:, top:bottom, left:right]\n",
    "                self.map_tiles.append(tile)\n",
    "                self.tile_positions.append({'top': top, 'bottom': bottom, 'left': left, 'right': right})\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) * len(self.map_tiles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image and tile indices\n",
    "        img_idx = idx // len(self.map_tiles)\n",
    "        tile_idx = idx % len(self.map_tiles)\n",
    "        \n",
    "        # Load drone image\n",
    "        img_path = self.image_paths[img_idx]\n",
    "        drone_image = load_image(img_path, resize=(self.target_height, self.target_width))\n",
    "        \n",
    "        # Get ground truth\n",
    "        gt_x, gt_y, _ = get_ground_truth_positions(self.train_pos, img_path)\n",
    "        \n",
    "        # Get map tile\n",
    "        map_tile = self.map_tiles[tile_idx]\n",
    "        tile_pos = self.tile_positions[tile_idx]\n",
    "        \n",
    "        # Adjust ground truth for tile coordinates\n",
    "        adj_gt_x = gt_x - tile_pos['left']\n",
    "        adj_gt_y = gt_y - tile_pos['top']\n",
    "        \n",
    "        # Check if ground truth is within tile bounds\n",
    "        if (adj_gt_x < 0 or adj_gt_x >= map_tile.shape[2] or \n",
    "            adj_gt_y < 0 or adj_gt_y >= map_tile.shape[1]):\n",
    "            # If GT is outside tile, use tile center as fallback\n",
    "            adj_gt_x = map_tile.shape[2] / 2\n",
    "            adj_gt_y = map_tile.shape[1] / 2\n",
    "        \n",
    "        return {\n",
    "            'drone_image': drone_image,\n",
    "            'map_tile': map_tile,\n",
    "            'gt_x': torch.tensor(adj_gt_x, dtype=torch.float32),\n",
    "            'gt_y': torch.tensor(adj_gt_y, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_image_paths = sorted(TRAIN_IMAGE_PATH.iterdir())[:20]  # Use subset for training\n",
    "val_image_paths = sorted(TRAIN_IMAGE_PATH.iterdir())[20:25]  # Use subset for validation\n",
    "\n",
    "train_dataset = DroneMapDataset(\n",
    "    train_image_paths, train_pos, data_path, \n",
    "    target_height, target_width, MAP_ZOOM_FACTOR, \n",
    "    MAP_HEIGHT, MAP_WIDTH, device\n",
    ")\n",
    "\n",
    "val_dataset = DroneMapDataset(\n",
    "    val_image_paths, train_pos, data_path,\n",
    "    target_height, target_width, MAP_ZOOM_FACTOR,\n",
    "    MAP_HEIGHT, MAP_WIDTH, device\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Fixed LightGlueFinetuner with proper gradient handling\n",
    "class LightGlueFinetuner:\n",
    "    def __init__(self, extractor, matcher, device):\n",
    "        self.extractor = extractor\n",
    "        self.matcher = matcher\n",
    "        self.device = device\n",
    "        \n",
    "        # Ensure matcher is in training mode and has gradients enabled\n",
    "        self.matcher.train()\n",
    "        for param in self.matcher.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Keep extractor frozen but in eval mode\n",
    "        self.extractor.eval()\n",
    "        for param in self.extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def pose_prediction_loss(self, drone_image, map_tile, gt_x, gt_y):\n",
    "        \"\"\"Compute pose prediction loss with proper gradient handling\"\"\"\n",
    "        try:\n",
    "            # Ensure inputs are on the correct device\n",
    "            drone_image = drone_image.to(self.device)\n",
    "            map_tile = map_tile.to(self.device)\n",
    "            \n",
    "            # Convert gt to tensors with gradients if needed\n",
    "            if not isinstance(gt_x, torch.Tensor):\n",
    "                gt_x = torch.tensor(gt_x, dtype=torch.float32, device=self.device)\n",
    "            if not isinstance(gt_y, torch.Tensor):\n",
    "                gt_y = torch.tensor(gt_y, dtype=torch.float32, device=self.device)\n",
    "            \n",
    "            gt_x = gt_x.to(self.device)\n",
    "            gt_y = gt_y.to(self.device)\n",
    "            \n",
    "            # Extract features (no gradients for extractor)\n",
    "            with torch.no_grad():\n",
    "                feats0 = self.extractor.extract(drone_image.unsqueeze(0))\n",
    "                feats1 = self.extractor.extract(map_tile.unsqueeze(0))\n",
    "            \n",
    "            # Run matcher with gradients enabled\n",
    "            matches01 = self.matcher({\"image0\": feats0, \"image1\": feats1})\n",
    "            \n",
    "            # Remove batch dimension\n",
    "            feats0, feats1, matches01 = [rbd(x) for x in [feats0, feats1, matches01]]\n",
    "            \n",
    "            kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "            \n",
    "            if len(matches) == 0:\n",
    "                # No matches - return high loss that requires grad\n",
    "                return torch.tensor(1000.0, device=self.device, requires_grad=True, dtype=torch.float32)\n",
    "            \n",
    "            # Use matching scores/probabilities instead of hard indexing\n",
    "            if \"matching_scores0\" in matches01:\n",
    "                # Get matching scores and create soft assignment\n",
    "                match_scores = matches01[\"matching_scores0\"]  # Shape: [N0]\n",
    "                valid_matches_mask = matches[:, 0] < len(match_scores)\n",
    "                \n",
    "                if valid_matches_mask.sum() == 0:\n",
    "                    return torch.tensor(1000.0, device=self.device, requires_grad=True, dtype=torch.float32)\n",
    "                \n",
    "                # Filter valid matches\n",
    "                valid_matches = matches[valid_matches_mask]\n",
    "                valid_scores = match_scores[valid_matches[:, 0]]\n",
    "                \n",
    "                # Convert to probabilities (this maintains gradients)\n",
    "                match_weights = torch.softmax(valid_scores, dim=0)\n",
    "                \n",
    "                # Get corresponding map keypoints\n",
    "                matched_map_kpts = kpts1[valid_matches[:, 1]]  # Still no grad, but we'll weight them\n",
    "                \n",
    "                # Create a differentiable weighted average using the match scores\n",
    "                # The key is that match_weights has gradients from the matcher\n",
    "                pred_pos = torch.sum(match_weights.unsqueeze(1) * matched_map_kpts, dim=0)\n",
    "                \n",
    "            else:\n",
    "                # Fallback: create uniform weights that still allow gradient flow\n",
    "                num_matches = len(matches)\n",
    "                uniform_weights = torch.ones(num_matches, device=self.device, requires_grad=True) / num_matches\n",
    "                \n",
    "                # Get matched keypoints\n",
    "                matched_map_kpts = kpts1[matches[:, 1]]\n",
    "                \n",
    "                # Weighted sum (gradients flow through uniform_weights)\n",
    "                pred_pos = torch.sum(uniform_weights.unsqueeze(1) * matched_map_kpts, dim=0)\n",
    "            \n",
    "            gt_pos = torch.tensor([gt_x, gt_y], dtype=torch.float32, device=self.device)\n",
    "            \n",
    "            # Compute L2 loss - this should now maintain gradients\n",
    "            loss = torch.norm(pred_pos - gt_pos, p=2)\n",
    "            \n",
    "            print(f\"GRAD: {pred_pos.requires_grad}, loss grad: {loss.requires_grad}\")\n",
    "            \n",
    "            return loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in pose_prediction_loss: {e}\")\n",
    "            # return torch.tensor(1000.0, device=self.device, requires_grad=True, dtype=torch.float32)\n",
    "    \n",
    "    def finetune(self, train_dataset, val_dataset, num_epochs=3, learning_rate=1e-5, batch_size=4, save_path=None):\n",
    "        \"\"\"Finetune LightGlue matcher\"\"\"\n",
    "        \n",
    "        # Ensure proper training setup\n",
    "        torch.set_grad_enabled(True)\n",
    "        self.matcher.train()\n",
    "        self.extractor.eval()\n",
    "        \n",
    "        # Create data loaders with error handling\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "        \n",
    "        # Set up optimizer (only train the matcher, keep extractor frozen)\n",
    "        # Filter parameters that require gradients\n",
    "        trainable_params = [p for p in self.matcher.parameters() if p.requires_grad]\n",
    "        print(f\"Training {len(trainable_params)} parameters\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Train batches per epoch: {len(train_loader)}\")\n",
    "        print(f\"Val batches per epoch: {len(val_loader)}\")\n",
    "        \n",
    "        if len(trainable_params) == 0:\n",
    "            print(\"Warning: No trainable parameters found!\")\n",
    "            return {}\n",
    "        \n",
    "        optimizer = torch.optim.Adam(trainable_params, lr=learning_rate)\n",
    "        \n",
    "        # Training history\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_error': [],\n",
    "            'val_error': []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            self.matcher.train()\n",
    "            train_losses = []\n",
    "            train_errors = []\n",
    "            \n",
    "            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")):\n",
    "                try:\n",
    "                    optimizer.zero_grad()\n",
    "                    batch_loss = 0\n",
    "                    valid_samples = 0\n",
    "                    \n",
    "                    # Process each sample in the batch\n",
    "                    for i in range(batch['drone_image'].size(0)):\n",
    "                        # Get batch data\n",
    "                        drone_image = batch['drone_image'][i]\n",
    "                        map_tile = batch['map_tile'][i]\n",
    "                        gt_x = batch['gt_x'][i].item()\n",
    "                        gt_y = batch['gt_y'][i].item()\n",
    "                        \n",
    "                        # Compute loss for this sample\n",
    "                        loss = self.pose_prediction_loss(drone_image, map_tile, gt_x, gt_y)\n",
    "                        \n",
    "                        # Check if loss has gradients\n",
    "                        if loss.requires_grad:\n",
    "                            batch_loss += loss\n",
    "                            valid_samples += 1\n",
    "                    \n",
    "                    # Only backpropagate if we have valid samples\n",
    "                    if valid_samples > 0:\n",
    "                        # Average loss over valid samples\n",
    "                        batch_loss = batch_loss / valid_samples\n",
    "                        \n",
    "                        # Backward pass\n",
    "                        batch_loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        train_losses.append(batch_loss.detach().cpu().item())\n",
    "                        train_errors.append(batch_loss.detach().cpu().item())\n",
    "                    \n",
    "                    # Log progress every 20 batches\n",
    "                    if (batch_idx + 1) % 20 == 0:\n",
    "                        avg_loss = np.mean(train_losses[-20:]) if len(train_losses) >= 20 else np.mean(train_losses)\n",
    "                        print(f\"  Batch {batch_idx+1}/{len(train_loader)}: Avg Loss = {avg_loss:.3f}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in training batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Validation phase\n",
    "            self.matcher.eval()\n",
    "            val_losses = []\n",
    "            val_errors = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\")):\n",
    "                    try:\n",
    "                        batch_loss = 0\n",
    "                        valid_samples = 0\n",
    "                        \n",
    "                        # Process each sample in the batch\n",
    "                        for i in range(batch['drone_image'].size(0)):\n",
    "                            # Get batch data\n",
    "                            drone_image = batch['drone_image'][i]\n",
    "                            map_tile = batch['map_tile'][i]\n",
    "                            gt_x = batch['gt_x'][i].item()\n",
    "                            gt_y = batch['gt_y'][i].item()\n",
    "                            \n",
    "                            # Compute loss (no gradients in validation)\n",
    "                            loss = self.pose_prediction_loss(drone_image, map_tile, gt_x, gt_y)\n",
    "                            \n",
    "                            batch_loss += loss.cpu().item()\n",
    "                            valid_samples += 1\n",
    "                        \n",
    "                        if valid_samples > 0:\n",
    "                            avg_batch_loss = batch_loss / valid_samples\n",
    "                            val_losses.append(avg_batch_loss)\n",
    "                            val_errors.append(avg_batch_loss)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in validation batch {batch_idx}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Record epoch metrics\n",
    "            if train_losses and val_losses:\n",
    "                epoch_train_loss = np.mean(train_losses)\n",
    "                epoch_val_loss = np.mean(val_losses)\n",
    "                epoch_train_error = np.mean(train_errors)\n",
    "                epoch_val_error = np.mean(val_errors)\n",
    "                \n",
    "                history['train_loss'].append(epoch_train_loss)\n",
    "                history['val_loss'].append(epoch_val_loss)\n",
    "                history['train_error'].append(epoch_train_error)\n",
    "                history['val_error'].append(epoch_val_error)\n",
    "                \n",
    "                print(f\"\\n=== Epoch {epoch+1}/{num_epochs} Summary ===\")\n",
    "                print(f\"  Train Loss: {epoch_train_loss:.3f}, Val Loss: {epoch_val_loss:.3f}\")\n",
    "                print(f\"  Train Error: {epoch_train_error:.3f}, Val Error: {epoch_val_error:.3f}\")\n",
    "                print(f\"  Train batches processed: {len(train_losses)}\")\n",
    "                print(f\"  Val batches processed: {len(val_losses)}\")\n",
    "                \n",
    "                # Check for improvement\n",
    "                if epoch > 0:\n",
    "                    train_improvement = history['train_loss'][-2] - epoch_train_loss\n",
    "                    val_improvement = history['val_loss'][-2] - epoch_val_loss\n",
    "                    print(f\"  Train improvement: {train_improvement:+.3f}\")\n",
    "                    print(f\"  Val improvement: {val_improvement:+.3f}\")\n",
    "                \n",
    "                print(\"=\" * 50)\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}: No valid losses computed\")\n",
    "        \n",
    "        # Save model if path provided\n",
    "        if save_path:\n",
    "            torch.save(self.matcher.state_dict(), save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "        \n",
    "        # Plot training history\n",
    "        if history['train_loss']:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "            \n",
    "            # Plot losses\n",
    "            epochs = range(1, len(history['train_loss']) + 1)\n",
    "            ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss')\n",
    "            ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Training and Validation Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True)\n",
    "            \n",
    "            # Plot errors\n",
    "            ax2.plot(epochs, history['train_error'], 'b-', label='Train Error')\n",
    "            ax2.plot(epochs, history['val_error'], 'r-', label='Val Error')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Error')\n",
    "            ax2.set_title('Training and Validation Error')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return history\n",
    "\n",
    "print(\"Fixed LightGlueFinetuner defined successfully!\")\n",
    "\n",
    "# Re-initialize the finetuner with proper gradient setup\n",
    "try:\n",
    "    # Enable gradients globally\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    # Create new finetuner instance\n",
    "    finetuner = LightGlueFinetuner(\n",
    "        extractor=extractor,\n",
    "        matcher=matcher,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(\"Starting finetuning with improved logging and larger batch size...\")\n",
    "    history = finetuner.finetune(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        num_epochs=3,\n",
    "        learning_rate=1e-5,\n",
    "        batch_size=8,  # Increased batch size\n",
    "        save_path='lightglue_finetuned.pth'\n",
    "    )\n",
    "    \n",
    "    print(\"Finetuning completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during finetuning: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy example\n",
    "The top image shows the matches, while the bottom image shows the point pruning across layers. In this case, LightGlue prunes a few points with occlusions, but is able to stop the context aggregation after 4/9 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image0 = load_image(images / \"DSC_0411.JPG\")\n",
    "image1 = load_image(images / \"DSC_0410.JPG\")\n",
    "\n",
    "feats0 = extractor.extract(image0.to(device))\n",
    "feats1 = extractor.extract(image1.to(device))\n",
    "matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n",
    "feats0, feats1, matches01 = [\n",
    "    rbd(x) for x in [feats0, feats1, matches01]\n",
    "]  # remove batch dimension\n",
    "\n",
    "kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "\n",
    "axes = viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_matches(m_kpts0, m_kpts1, color=\"lime\", lw=0.2)\n",
    "viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers', fs=20)\n",
    "\n",
    "kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficult example\n",
    "For pairs with significant viewpoint- and illumination changes, LightGlue can exclude a lot of points early in the matching process (red points), which significantly reduces the inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image0 = load_image(images / \"sacre_coeur1.jpg\")\n",
    "image1 = load_image(images / \"sacre_coeur2.jpg\")\n",
    "\n",
    "feats0 = extractor.extract(image0.to(device))\n",
    "feats1 = extractor.extract(image1.to(device))\n",
    "matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n",
    "feats0, feats1, matches01 = [\n",
    "    rbd(x) for x in [feats0, feats1, matches01]\n",
    "]  # remove batch dimension\n",
    "\n",
    "kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "\n",
    "axes = viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_matches(m_kpts0, m_kpts1, color=\"lime\", lw=0.2)\n",
    "viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers')\n",
    "\n",
    "kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".se3loc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
