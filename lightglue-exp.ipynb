{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGlue Based Image-Map Matching\n",
    "this notebook is based on the [LightGlue Demo](https://github.com/cvg/LightGlue/blob/main/demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are on colab: this clones the repo and installs the dependencies\n",
    "from pathlib import Path\n",
    "\n",
    "# if Path.cwd().name != \"LightGlue\":\n",
    "#     if not Path(\"LightGlue\").exists():\n",
    "#         !git clone --quiet https://github.com/cvg/LightGlue/\n",
    "#     %cd LightGlue\n",
    "    # !pip install --progress-bar off --quiet -e .\n",
    "\n",
    "from lightglue import LightGlue, SuperPoint, DISK\n",
    "from lightglue.utils import load_image, rbd\n",
    "from lightglue import viz2d\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "images = Path(\"assets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load extractor and matcher module\n",
    "In this example we use SuperPoint features combined with LightGlue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability and set device appropriately\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")  \n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_num_keypoints=2048\n",
    "max_num_keypoints = None\n",
    "extractor = SuperPoint(max_num_keypoints = max_num_keypoints).eval().to(device)  # load the extractor\n",
    "matcher = LightGlue(features=\"superpoint\").eval().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/home/andrii/edth/2/gnss-denied-localization-munich/data\")\n",
    "# Load images and move to device (works for both CUDA and CPU)\n",
    "image_map = load_image(data_path / 'map.png').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training positions\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "train_pos = pd.read_csv(data_path / 'train_data/train_pos.csv')\n",
    "\n",
    "def extract_and_match(image0, image_map, map_features=None):\n",
    "    feats0 = extractor.extract(image0)\n",
    "    if map_features is not None:\n",
    "        feats1 = map_features\n",
    "    else:\n",
    "        feats1 = extractor.extract(image_map)\n",
    "    matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n",
    "    feats0, feats1, matches01 = [\n",
    "        rbd(x) for x in [feats0, feats1, matches01]\n",
    "    ]  # remove batch dimension\n",
    "\n",
    "    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "    # print(f\"__Number of matches: {len(m_kpts0)}\")\n",
    "    return m_kpts0, m_kpts1, matches01, kpts0, kpts1\n",
    "\n",
    "def get_ground_truth_positions(train_pos, img_path):\n",
    "    # Get id from filename\n",
    "    id = int(img_path.name.split('.')[0])\n",
    "    # Get the ground truth positions for the current image\n",
    "    gt_positions = train_pos[train_pos['id'] == id]\n",
    "    gt_x = gt_positions.iloc[0]['x_pixel']\n",
    "    gt_y = gt_positions.iloc[0]['y_pixel']\n",
    "    return gt_x, gt_y, id\n",
    "\n",
    "def get_zoomed_map(image_map, gt_x, gt_y, zoom_factor=5):\n",
    "    # Get map dimensions\n",
    "    map_height, map_width = image_map.shape[1], image_map.shape[2]\n",
    "    \n",
    "    # Calculate zoom window (4x zoom means 1/4 of original size)\n",
    "    window_width = map_width // zoom_factor\n",
    "    window_height = map_height // zoom_factor\n",
    "    \n",
    "    # Calculate crop boundaries centered on ground truth\n",
    "    left = max(0, int(gt_x - window_width // 2))\n",
    "    right = min(map_width, int(gt_x + window_width // 2))\n",
    "    top = max(0, int(gt_y - window_height // 2))\n",
    "    bottom = min(map_height, int(gt_y + window_height // 2))\n",
    "    \n",
    "    # Crop the map\n",
    "    zoomed_map = image_map[:, top:bottom, left:right]\n",
    "\n",
    "    # Adjust ground truth coordinates for the cropped image\n",
    "    adj_gt_x = gt_x - left\n",
    "    adj_gt_y = gt_y - top\n",
    "    \n",
    "    return zoomed_map, adj_gt_x, adj_gt_y\n",
    "\n",
    "def get_map_tile_features(image_map, zoom_factor=5):\n",
    "    \"\"\"\n",
    "    Extract features for map tiles at a specified zoom factor, creating a grid of tiles across the entire map.\n",
    "    \"\"\"\n",
    "    # Get map dimensions\n",
    "    map_height, map_width = image_map.shape[1], image_map.shape[2]\n",
    "    \n",
    "    # Calculate tile dimensions\n",
    "    tile_width = map_width // zoom_factor\n",
    "    tile_height = map_height // zoom_factor\n",
    "    \n",
    "    tile_features = []\n",
    "    tile_positions = []\n",
    "    tiles = []\n",
    "    \n",
    "    # Create grid of tiles\n",
    "    for row in range(zoom_factor):\n",
    "        for col in range(zoom_factor):\n",
    "            # Calculate tile boundaries\n",
    "            left = col * tile_width\n",
    "            right = min((col + 1) * tile_width, map_width)\n",
    "            top = row * tile_height\n",
    "            bottom = min((row + 1) * tile_height, map_height)\n",
    "            \n",
    "            # Extract tile\n",
    "            tile = image_map[:, top:bottom, left:right]\n",
    "            \n",
    "            # Extract features from the tile\n",
    "            feats = extractor.extract(tile)\n",
    "            feats = rbd(feats)  # remove batch dimension\n",
    "            \n",
    "            tile_features.append(feats)\n",
    "            tile_positions.append({\n",
    "                'row': row, \n",
    "                'col': col, \n",
    "                'left': left, \n",
    "                'right': right, \n",
    "                'top': top, \n",
    "                'bottom': bottom\n",
    "            })\n",
    "            tiles.append(tile)\n",
    "    \n",
    "    return tile_features, tile_positions, tiles\n",
    "\n",
    "def plot_matches(image0, img_path, image_map, train_pos, zoom_factor=5, map_features=None, plot_gt=False):\n",
    "    gt_x, gt_y, id = get_ground_truth_positions(train_pos, img_path)\n",
    "    print(f\"__Plotting ground truth for ID: {id}\")\n",
    "    print(f\"__Ground truth position: ({gt_x:.1f}, {gt_y:.1f})\")\n",
    "\n",
    "    # Get zoomed map and adjusted ground truth coordinates\n",
    "    if zoom_factor > 1:\n",
    "        zoomed_map, adj_gt_x, adj_gt_y = get_zoomed_map(image_map, gt_x, gt_y, zoom_factor)\n",
    "        image_map = zoomed_map.to(device)\n",
    "        gt_x, gt_y = adj_gt_x, adj_gt_y\n",
    "\n",
    "    # Extract and match keypoints\n",
    "    m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(image0, image_map, map_features)\n",
    "    # Visualize the images\n",
    "    _ = viz2d.plot_images([image0, image_map], titles=[f\"Image {id}\", f\"Map (Zoom {zoom_factor}x)\"])\n",
    "    # Get the current figure and axes\n",
    "    fig = plt.gcf()\n",
    "    axes = fig.axes\n",
    "    \n",
    "    # Plot red dot on the map (image1) at ground truth position\n",
    "    if plot_gt:\n",
    "        axes[1].plot(gt_x, gt_y, 'ro', markersize=8, markeredgecolor='white', markeredgewidth=1)\n",
    "        axes[1].text(gt_x + 20, gt_y - 20, f'GT ({gt_x:.0f}, {gt_y:.0f})', \n",
    "                    color='red', fontsize=10, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    # Plot matches\n",
    "    viz2d.plot_matches(m_kpts0, m_kpts1, color=\"blue\", lw=0.3)\n",
    "    viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers')\n",
    "    \n",
    "    # Compute center of map matches and plot\n",
    "    if len(m_kpts1) > 0:\n",
    "        center_x = m_kpts1[:, 0].mean().item()\n",
    "        center_y = m_kpts1[:, 1].mean().item()\n",
    "        axes[1].plot(center_x, center_y, 'go', markersize=10, markeredgecolor='white', markeredgewidth=2)\n",
    "        axes[1].text(center_x + 20, center_y + 20, f'Center ({center_x:.0f}, {center_y:.0f})', \n",
    "                    color='green', fontsize=10, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return center_x, center_y\n",
    "\n",
    "    # # Plot pruned keypoints\n",
    "    # kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "    # viz2d.plot_images([image0, image_map])\n",
    "    # viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=6)\n",
    "\n",
    "\n",
    "def compute_pose_prediction(image0, img_path, image_map, train_pos, zoom_factor=5, map_features=None, plot_gt=False):\n",
    "    # Extract and match keypoints\n",
    "    m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(image0, image_map, map_features)\n",
    "    \n",
    "    # Compute center of map matches and plot\n",
    "    if len(m_kpts1) > 0:\n",
    "        center_x = m_kpts1[:, 0].mean().item()\n",
    "        center_y = m_kpts1[:, 1].mean().item()\n",
    "\n",
    "    return center_x, center_y\n",
    "\n",
    "def compute_pose_prediction_with_transform(image0, img_path, image_map, train_pos, zoom_factor=5, map_features=None, plot_gt=False):\n",
    "    \"\"\"\n",
    "    Compute pose prediction using geometric transformation between matched keypoints.\n",
    "    Estimates translation, rotation, and scale, then applies to image center.\n",
    "    \"\"\"\n",
    "    # Extract and match keypoints\n",
    "    m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(image0, image_map, map_features)\n",
    "    \n",
    "    if len(m_kpts1) < 4:  # Need at least 4 points for robust transform estimation\n",
    "        # Fallback to centroid method\n",
    "        if len(m_kpts1) > 0:\n",
    "            center_x = m_kpts1[:, 0].mean().item()\n",
    "            center_y = m_kpts1[:, 1].mean().item()\n",
    "            return center_x, center_y, None\n",
    "        else:\n",
    "            # No matches - return map center\n",
    "            map_height, map_width = image_map.shape[1], image_map.shape[2]\n",
    "            return map_width // 2, map_height // 2, None\n",
    "    \n",
    "    # Convert to numpy arrays for OpenCV\n",
    "    pts_drone = m_kpts0.cpu().numpy().astype(np.float32)\n",
    "    pts_map = m_kpts1.cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # Method 1: Estimate similarity transform (translation + rotation + uniform scale)\n",
    "    try:\n",
    "        # Use RANSAC to find robust similarity transform\n",
    "        transform_matrix, inliers = cv2.estimateAffinePartial2D(\n",
    "            pts_drone, pts_map, \n",
    "            method=cv2.RANSAC,\n",
    "            ransacReprojThreshold=10.0,  # Adjust based on expected accuracy\n",
    "            maxIters=2000,\n",
    "            confidence=0.99\n",
    "        )\n",
    "        \n",
    "        if transform_matrix is not None and inliers is not None:\n",
    "            # Get image center in drone coordinates\n",
    "            drone_height, drone_width = image0.shape[1], image0.shape[2]\n",
    "            drone_center = np.array([[drone_width // 2, drone_height // 2]], dtype=np.float32)\n",
    "            \n",
    "            # Transform drone center to map coordinates\n",
    "            map_center = cv2.transform(drone_center.reshape(1, 1, 2), transform_matrix)\n",
    "            center_x, center_y = map_center[0, 0, 0], map_center[0, 0, 1]\n",
    "            \n",
    "            # Extract transformation parameters\n",
    "            transform_info = {\n",
    "                'matrix': transform_matrix,\n",
    "                'translation': [transform_matrix[0, 2], transform_matrix[1, 2]],\n",
    "                'scale': np.sqrt(transform_matrix[0, 0]**2 + transform_matrix[0, 1]**2),\n",
    "                'rotation_deg': np.degrees(np.arctan2(transform_matrix[1, 0], transform_matrix[0, 0])),\n",
    "                'inliers_count': np.sum(inliers),\n",
    "                'inlier_ratio': np.sum(inliers) / len(inliers)\n",
    "            }\n",
    "            \n",
    "            return center_x, center_y, transform_info\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Similarity transform failed: {e}\")\n",
    "    \n",
    "    # Method 2: Fallback to homography (more flexible but can be less stable)\n",
    "    try:\n",
    "        homography, mask = cv2.findHomography(\n",
    "            pts_drone, pts_map,\n",
    "            cv2.RANSAC,\n",
    "            ransacReprojThreshold=10.0\n",
    "        )\n",
    "        \n",
    "        if homography is not None:\n",
    "            # Transform drone center using homography\n",
    "            drone_height, drone_width = image0.shape[1], image0.shape[2]\n",
    "            drone_center = np.array([[[drone_width // 2, drone_height // 2]]], dtype=np.float32)\n",
    "            \n",
    "            map_center = cv2.perspectiveTransform(drone_center, homography)\n",
    "            center_x, center_y = map_center[0, 0, 0], map_center[0, 0, 1]\n",
    "            \n",
    "            transform_info = {\n",
    "                'matrix': homography,\n",
    "                'method': 'homography',\n",
    "                'inliers_count': np.sum(mask),\n",
    "                'inlier_ratio': np.sum(mask) / len(mask) if mask is not None else 0\n",
    "            }\n",
    "            \n",
    "            return center_x, center_y, transform_info\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Homography failed: {e}\")\n",
    "    \n",
    "    # Method 3: Final fallback to weighted centroid\n",
    "    if len(m_kpts1) > 0:\n",
    "        # Use match confidence if available\n",
    "        if \"matching_scores0\" in matches01:\n",
    "            matches = matches01[\"matches\"]\n",
    "            scores = matches01[\"matching_scores0\"][matches[..., 0]]\n",
    "            weights = torch.softmax(scores, dim=0).cpu().numpy()\n",
    "            \n",
    "            center_x = np.average(m_kpts1[:, 0].cpu().numpy(), weights=weights)\n",
    "            center_y = np.average(m_kpts1[:, 1].cpu().numpy(), weights=weights)\n",
    "        else:\n",
    "            center_x = m_kpts1[:, 0].mean().item()\n",
    "            center_y = m_kpts1[:, 1].mean().item()\n",
    "        \n",
    "        transform_info = {\n",
    "            'method': 'weighted_centroid',\n",
    "            'num_matches': len(m_kpts1)\n",
    "        }\n",
    "        \n",
    "        return center_x, center_y, transform_info\n",
    "    \n",
    "    # Ultimate fallback - map center\n",
    "    map_height, map_width = image_map.shape[1], image_map.shape[2]\n",
    "    return map_width // 2, map_height // 2, {'method': 'map_center'}\n",
    "\n",
    "\n",
    "def compute_pose_prediction_robust(image0, img_path, image_map, train_pos, zoom_factor=5, map_features=None, plot_gt=False):\n",
    "    \"\"\"\n",
    "    Enhanced version with multiple geometric estimation methods and quality assessment.\n",
    "    \"\"\"\n",
    "    # Extract and match keypoints\n",
    "    m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(image0, image_map, map_features)\n",
    "    \n",
    "    if len(m_kpts1) < 3:\n",
    "        # Not enough matches for geometric estimation\n",
    "        if len(m_kpts1) > 0:\n",
    "            center_x = m_kpts1[:, 0].mean().item()\n",
    "            center_y = m_kpts1[:, 1].mean().item()\n",
    "        else:\n",
    "            map_height, map_width = image_map.shape[1], image_map.shape[2]\n",
    "            center_x, center_y = map_width // 2, map_height // 2\n",
    "        \n",
    "        return center_x, center_y, {'method': 'simple_centroid', 'num_matches': len(m_kpts1)}\n",
    "    \n",
    "    # Convert to numpy\n",
    "    pts_drone = m_kpts0.cpu().numpy().astype(np.float32)\n",
    "    pts_map = m_kpts1.cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # Get drone image center\n",
    "    drone_height, drone_width = image0.shape[1], image0.shape[2]\n",
    "    drone_center = np.array([drone_width // 2, drone_height // 2], dtype=np.float32)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Method 1: Similarity Transform (best for drone imagery)\n",
    "    if len(m_kpts1) >= 2:\n",
    "        try:\n",
    "            # Estimate similarity transform with RANSAC\n",
    "            sim_matrix, inliers = cv2.estimateAffinePartial2D(\n",
    "                pts_drone, pts_map,\n",
    "                method=cv2.RANSAC,\n",
    "                ransacReprojThreshold=15.0,\n",
    "                maxIters=1000,\n",
    "                confidence=0.95\n",
    "            )\n",
    "            \n",
    "            if sim_matrix is not None and inliers is not None:\n",
    "                # Transform drone center\n",
    "                transformed_center = cv2.transform(\n",
    "                    drone_center.reshape(1, 1, 2), sim_matrix\n",
    "                )[0, 0]\n",
    "                \n",
    "                inlier_count = np.sum(inliers)\n",
    "                inlier_ratio = inlier_count / len(inliers)\n",
    "                \n",
    "                # Calculate transform parameters\n",
    "                tx, ty = sim_matrix[0, 2], sim_matrix[1, 2]\n",
    "                scale = np.sqrt(sim_matrix[0, 0]**2 + sim_matrix[0, 1]**2)\n",
    "                rotation = np.degrees(np.arctan2(sim_matrix[1, 0], sim_matrix[0, 0]))\n",
    "                \n",
    "                quality_score = inlier_ratio * min(inlier_count / 10.0, 1.0)  # Prefer more inliers\n",
    "                \n",
    "                results.append({\n",
    "                    'center': transformed_center,\n",
    "                    'method': 'similarity_transform',\n",
    "                    'quality': quality_score,\n",
    "                    'inliers': inlier_count,\n",
    "                    'inlier_ratio': inlier_ratio,\n",
    "                    'translation': [tx, ty],\n",
    "                    'scale': scale,\n",
    "                    'rotation_deg': rotation,\n",
    "                    'matrix': sim_matrix\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Method 2: Least Squares Rigid Transform (translation + rotation only)\n",
    "    if len(m_kpts1) >= 2:\n",
    "        try:\n",
    "            # Compute centroids\n",
    "            centroid_drone = np.mean(pts_drone, axis=0)\n",
    "            centroid_map = np.mean(pts_map, axis=0)\n",
    "            \n",
    "            # Center the points\n",
    "            pts_drone_centered = pts_drone - centroid_drone\n",
    "            pts_map_centered = pts_map - centroid_map\n",
    "            \n",
    "            # Compute rotation using SVD\n",
    "            H = pts_drone_centered.T @ pts_map_centered\n",
    "            U, S, Vt = np.linalg.svd(H)\n",
    "            R = Vt.T @ U.T\n",
    "            \n",
    "            # Ensure proper rotation (det(R) = 1)\n",
    "            if np.linalg.det(R) < 0:\n",
    "                Vt[-1, :] *= -1\n",
    "                R = Vt.T @ U.T\n",
    "            \n",
    "            # Compute translation\n",
    "            t = centroid_map - R @ centroid_drone\n",
    "            \n",
    "            # Transform drone center\n",
    "            transformed_center = R @ drone_center + t\n",
    "            \n",
    "            # Compute residual error\n",
    "            transformed_pts = (R @ pts_drone.T).T + t\n",
    "            errors = np.linalg.norm(transformed_pts - pts_map, axis=1)\n",
    "            mean_error = np.mean(errors)\n",
    "            \n",
    "            quality_score = max(0, 1.0 - mean_error / 50.0)  # Normalize error\n",
    "            \n",
    "            results.append({\n",
    "                'center': transformed_center,\n",
    "                'method': 'rigid_transform',\n",
    "                'quality': quality_score,\n",
    "                'mean_error': mean_error,\n",
    "                'rotation_matrix': R,\n",
    "                'translation': t,\n",
    "                'rotation_deg': np.degrees(np.arctan2(R[1, 0], R[0, 0]))\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Method 3: Weighted centroid (always available)\n",
    "    if \"matching_scores0\" in matches01:\n",
    "        matches = matches01[\"matches\"]\n",
    "        scores = matches01[\"matching_scores0\"][matches[..., 0]]\n",
    "        weights = torch.softmax(scores, dim=0).cpu().numpy()\n",
    "        \n",
    "        center_x = np.average(m_kpts1[:, 0].cpu().numpy(), weights=weights)\n",
    "        center_y = np.average(m_kpts1[:, 1].cpu().numpy(), weights=weights)\n",
    "    else:\n",
    "        center_x = m_kpts1[:, 0].mean().item()\n",
    "        center_y = m_kpts1[:, 1].mean().item()\n",
    "    \n",
    "    results.append({\n",
    "        'center': np.array([center_x, center_y]),\n",
    "        'method': 'weighted_centroid',\n",
    "        'quality': 0.5,  # Medium confidence\n",
    "        'num_matches': len(m_kpts1)\n",
    "    })\n",
    "    \n",
    "    # Select best result based on quality score\n",
    "    best_result = max(results, key=lambda x: x['quality'])\n",
    "    \n",
    "    return best_result['center'][0], best_result['center'][1], best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGE_PATH = data_path / \"test_data\" / \"test_images\"\n",
    "TRAIN_IMAGE_PATH = data_path / \"train_data\" / \"train_images\"\n",
    "\n",
    "image_paths = sorted(TRAIN_IMAGE_PATH.iterdir())\n",
    "\n",
    "# Configuration parameters\n",
    "DRONE_SCALE_FACTOR = 8  # Scale factor for drone image resizing\n",
    "MAP_ZOOM_FACTOR = int(DRONE_SCALE_FACTOR * 0.625)     # Map tiling factor\n",
    "STRIDE_FACTOR = 2  # 2 = 50% overlap; 1 = stride equals window size\n",
    "\n",
    "# Known resolutions\n",
    "DRONE_WIDTH = 8004\n",
    "DRONE_HEIGHT = 6001\n",
    "MAP_WIDTH = 5000\n",
    "MAP_HEIGHT = 2500\n",
    "\n",
    "NUM_BEST_TILES = 5\n",
    "\n",
    "# Calculate target resize dimensions\n",
    "target_width = DRONE_WIDTH // DRONE_SCALE_FACTOR   # 8004 // 6 = 1334\n",
    "target_height = DRONE_HEIGHT // DRONE_SCALE_FACTOR # 6001 // 6 = 1000\n",
    "\n",
    "window_height, window_width = MAP_HEIGHT // MAP_ZOOM_FACTOR, MAP_WIDTH // MAP_ZOOM_FACTOR\n",
    "stride_y, stride_x = window_height // STRIDE_FACTOR, window_width // STRIDE_FACTOR\n",
    "\n",
    "print(f\"Drone image scaling: {DRONE_WIDTH}x{DRONE_HEIGHT} -> {target_width}x{target_height} (factor: {DRONE_SCALE_FACTOR})\")\n",
    "print(f\"Map dimensions: {MAP_WIDTH}x{MAP_HEIGHT} -> Tile size: {window_width}x{window_height} (zoom factor: {MAP_ZOOM_FACTOR})\")\n",
    "print(f\"Stride: {stride_x}x{stride_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for x images in train_data with a step of 50\n",
    "for i in range(0, len(image_paths), 70):\n",
    "    img_path = image_paths[i]\n",
    "    print(f\"Processing image: {img_path.name}...\")\n",
    "    \n",
    "    # Resize drone image using calculated dimensions\n",
    "    image0 = load_image(img_path, resize=(target_height, target_width)).to(device)\n",
    "    \n",
    "    # Store tile results with match counts\n",
    "    tile_results = []\n",
    "    \n",
    "    # Generate all 90-degree rotations of the image (0, 90, 180, 270 degrees)\n",
    "    rotated_images = [image0]\n",
    "    for k in range(1, 4):\n",
    "        rotated_images.append(torch.rot90(image0, k, dims=[1, 2]))\n",
    "    for rot_idx, rotated_image in enumerate(rotated_images):\n",
    "\n",
    "        windows = []\n",
    "        map_tile_positions = []\n",
    "\n",
    "        for top in range(0, MAP_HEIGHT - window_height + 1, stride_y):\n",
    "            for left in range(0, MAP_WIDTH - window_width + 1, stride_x):\n",
    "                bottom = top + window_height\n",
    "                right = left + window_width\n",
    "                window = image_map[:, top:bottom, left:right]\n",
    "                windows.append(window)\n",
    "                map_tile_positions.append(\n",
    "                    {\"top\": top,\n",
    "                    \"bottom\": bottom,\n",
    "                    \"left\": left,\n",
    "                    \"right\": right}\n",
    "                    )\n",
    "        \n",
    "        for tile_pos, tile_img in zip(map_tile_positions, windows):\n",
    "            # Extract and match to count matches without plotting\n",
    "            m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(rotated_image, tile_img, map_features=None)\n",
    "            num_matches = len(m_kpts0)\n",
    "            \n",
    "            # Store results\n",
    "            tile_results.append({\n",
    "                'tile_pos': tile_pos,\n",
    "                'tile_img': tile_img,\n",
    "                'num_matches': num_matches,\n",
    "                'matches_data': (m_kpts0, m_kpts1, matches01, kpts0, kpts1),\n",
    "                'rotation_index': rot_idx,\n",
    "                'rotated_image': rotated_image\n",
    "            })\n",
    "            \n",
    "            # print(f\"__Tile [{tile_pos[\"left\"]}, {tile_pos[\"top\"]}] has {num_matches} matches\")\n",
    "        \n",
    "    # Sort by number of matches (descending) and take top 3\n",
    "    best_tiles = sorted(tile_results, key=lambda x: x['num_matches'], reverse=True)[:NUM_BEST_TILES]\n",
    "    \n",
    "    print(f\"\\n=== Top 3 tiles for image {img_path.name} ===\")\n",
    "    for idx, tile_result in enumerate(best_tiles, 1):\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        num_matches = tile_result['num_matches']\n",
    "        print(f\"{idx}. Tile {tile_pos['left']}, {tile_pos['top']}: {num_matches} matches\")\n",
    "    \n",
    "    # Plot only the best 3 tiles\n",
    "    for idx, tile_result in enumerate(best_tiles, 1):\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        tile_img = tile_result['tile_img']\n",
    "        num_matches = tile_result['num_matches']\n",
    "        best_rotation_idx = tile_result['rotation_index']\n",
    "        rotated_image = tile_result['rotated_image']\n",
    "        \n",
    "        print(f\"\\n--- Plotting #{idx} best tile (image rot= {best_rotation_idx *90}) at: [{tile_pos['left']}, {tile_pos['top']}] with {num_matches} matches ---\")\n",
    "        pred_x, pred_y = plot_matches(rotated_image, img_path, tile_img, train_pos, \n",
    "                                      zoom_factor=1, map_features=None, plot_gt=False)\n",
    "        \n",
    "        # Plot whole map with best tile highlighted, predicted position, and ground truth\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.imshow(image_map.cpu().numpy().transpose(1, 2, 0))\n",
    "        \n",
    "        # Get ground truth position for the current image\n",
    "        gt_x, gt_y, id = get_ground_truth_positions(train_pos, img_path)\n",
    "        \n",
    "        # Highlight the best tile with a rectangle\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        rect = plt.Rectangle((tile_pos['left'], tile_pos['top']), \n",
    "                   tile_pos['right'] - tile_pos['left'], \n",
    "                   tile_pos['bottom'] - tile_pos['top'],\n",
    "                   linewidth=1, edgecolor='blue', facecolor='none', alpha=0.8)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Adjust predicted position to full map coordinates\n",
    "        full_map_pred_x = pred_x + tile_pos['left']\n",
    "        full_map_pred_y = pred_y + tile_pos['top']\n",
    "        \n",
    "        # Plot ground truth position (red dot)\n",
    "        ax.plot(gt_x, gt_y, 'ro', markersize=10, markeredgecolor='white', markeredgewidth=2)\n",
    "        ax.text(gt_x + 50, gt_y - 50, f'GT ({gt_x:.0f}, {gt_y:.0f})', \n",
    "            color='red', fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Plot predicted position (green dot)\n",
    "        ax.plot(full_map_pred_x, full_map_pred_y, 'go', markersize=10, markeredgecolor='white', markeredgewidth=2)\n",
    "        ax.text(full_map_pred_x + 50, full_map_pred_y + 50, f'Pred ({full_map_pred_x:.0f}, {full_map_pred_y:.0f})', \n",
    "            color='green', fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Add tile information\n",
    "        ax.text(tile_pos['left'] + 10, tile_pos['top'] + 30, \n",
    "            f'Best Tile \\n{num_matches} matches', \n",
    "            color='blue', fontsize=10, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        ax.set_title(f'Full Map - Image {id} - Tile Ranking #{idx}')\n",
    "        ax.set_xlabel('X coordinate')\n",
    "        ax.set_ylabel('Y coordinate')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 136 test images...\n",
      "Processing 1/136: 0001.JPG\n",
      "Processing 11/136: 0011.JPG\n",
      "Processing 21/136: 0066.JPG\n",
      "Processing 31/136: 2475.JPG\n",
      "Processing 41/136: 2485.JPG\n",
      "Processing 51/136: 2495.JPG\n",
      "Processing 61/136: 2505.JPG\n",
      "Processing 71/136: 2515.JPG\n",
      "Processing 81/136: 2525.JPG\n",
      "Processing 91/136: 2670.JPG\n",
      "Processing 101/136: 2710.JPG\n",
      "Processing 111/136: 2750.JPG\n",
      "Processing 121/136: 2790.JPG\n",
      "Processing 131/136: 2800.JPG\n"
     ]
    }
   ],
   "source": [
    "# Run on ALL test data and save to CSV\n",
    "TEST_IMAGE_PATH = data_path / \"test_data\" / \"test_images\"\n",
    "test_image_paths = sorted(TEST_IMAGE_PATH.iterdir())\n",
    "MIN_MATCHES_THRESHOLD = 10  # Minimum matches to consider a tile\n",
    "HIGH_CONFIDENCE_THRESHOLD = 40  # If a tile has this many matches, use only this tile\n",
    "NUM_BEST_TILES = 1\n",
    "print(f\"Processing {len(test_image_paths)} test images...\")\n",
    "\n",
    "# Prepare CSV file with headers\n",
    "output_path = data_path / 'predictions_lightglue_test.csv'\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write('id,x_pixel,y_pixel\\n')\n",
    "\n",
    "for i, img_path in enumerate(test_image_paths):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing {i+1}/{len(test_image_paths)}: {img_path.name}\")\n",
    "    \n",
    "    img_id = int(img_path.name.split('.')[0])\n",
    "    \n",
    "    # Resize drone image using calculated dimensions\n",
    "    image0 = load_image(img_path, resize=(target_height, target_width)).to(device)\n",
    "    \n",
    "    # Store tile results with match counts\n",
    "    tile_results = []\n",
    "    \n",
    "    # Generate all 90-degree rotations of the image (0, 90, 180, 270 degrees)\n",
    "    rotated_images = [image0]\n",
    "    for k in range(1, 4):\n",
    "        rotated_images.append(torch.rot90(image0, k, dims=[1, 2]))\n",
    "    for rot_idx, rotated_image in enumerate(rotated_images):\n",
    "    \n",
    "        # Sliding window approach\n",
    "        windows = []\n",
    "        map_tile_positions = []\n",
    "\n",
    "        for top in range(0, MAP_HEIGHT - window_height + 1, stride_y):\n",
    "            for left in range(0, MAP_WIDTH - window_width + 1, stride_x):\n",
    "                bottom = top + window_height\n",
    "                right = left + window_width\n",
    "                window = image_map[:, top:bottom, left:right]\n",
    "                windows.append(window)\n",
    "                map_tile_positions.append(\n",
    "                    {\"top\": top,\n",
    "                    \"bottom\": bottom,\n",
    "                    \"left\": left,\n",
    "                    \"right\": right}\n",
    "                    )\n",
    "        \n",
    "        for tile_pos, tile_img in zip(map_tile_positions, windows):\n",
    "            # Extract and match to count matches without plotting\n",
    "            m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(rotated_image, tile_img, map_features=None)\n",
    "            num_matches = len(m_kpts0)\n",
    "            \n",
    "            # Store results\n",
    "            tile_results.append({\n",
    "                'tile_pos': tile_pos,\n",
    "                'tile_img': tile_img,\n",
    "                'num_matches': num_matches,\n",
    "                'matches_data': (m_kpts0, m_kpts1, matches01, kpts0, kpts1),\n",
    "                'rotation_index': rot_idx,\n",
    "                'rotated_image': rotated_image\n",
    "            })\n",
    "        \n",
    "     # Sort by number of matches (descending) and apply filtering logic\n",
    "    sorted_tiles = sorted(tile_results, key=lambda x: x['num_matches'], reverse=True)\n",
    "\n",
    "    # Check if any tiles have high confidence\n",
    "    high_confidence_tiles = [tile for tile in sorted_tiles if tile['num_matches'] >= HIGH_CONFIDENCE_THRESHOLD]\n",
    "\n",
    "    if len(high_confidence_tiles) > 0:\n",
    "        # Use all high confidence tiles\n",
    "        best_tiles = high_confidence_tiles\n",
    "    else:\n",
    "        # Filter tiles with enough matches\n",
    "        good_tiles = [tile for tile in sorted_tiles if tile['num_matches'] >= MIN_MATCHES_THRESHOLD]\n",
    "        \n",
    "        if len(good_tiles) > 0:\n",
    "            # Use up to NUM_BEST_TILES from good tiles\n",
    "            best_tiles = good_tiles[:NUM_BEST_TILES]\n",
    "        else:\n",
    "            # If no tile meets minimum threshold, use at least the best one\n",
    "            best_tiles = [sorted_tiles[0]]\n",
    "            \n",
    "    # Process multiple best tiles and collect predictions\n",
    "    predictions_x = []\n",
    "    predictions_y = []\n",
    "    \n",
    "    for idx, tile_result in enumerate(best_tiles, 1):\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        tile_img = tile_result['tile_img']\n",
    "        num_matches = tile_result['num_matches']\n",
    "        best_rotation_idx = tile_result['rotation_index']\n",
    "        rotated_image = tile_result['rotated_image']\n",
    "        \n",
    "        # Only use tiles with at least some matches\n",
    "        if num_matches > 0:\n",
    "            pred_x, pred_y = compute_pose_prediction(rotated_image, img_path, tile_img, train_pos, \n",
    "                                          zoom_factor=1, map_features=None, plot_gt=False)\n",
    "            \n",
    "            # Adjust predicted position to full map coordinates\n",
    "            full_map_pred_x = pred_x + tile_pos['left']\n",
    "            full_map_pred_y = pred_y + tile_pos['top']\n",
    "            \n",
    "            predictions_x.append(full_map_pred_x)\n",
    "            predictions_y.append(full_map_pred_y)\n",
    "    \n",
    "    # Calculate mean prediction from multiple tiles\n",
    "    if predictions_x:\n",
    "        final_pred_x = sum(predictions_x) / len(predictions_x)\n",
    "        final_pred_y = sum(predictions_y) / len(predictions_y)\n",
    "        num_tiles_used = len(predictions_x)\n",
    "    \n",
    "    # Write prediction to CSV immediately\n",
    "    with open(output_path, 'a') as f:\n",
    "        f.write(f'{img_id},{full_map_pred_x:.1f},{full_map_pred_y:.1f}\\n')\n",
    "\n",
    "print(f\"\\nSaved predictions to {output_path}\")\n",
    "\n",
    "# Quick validation\n",
    "predictions_df = pd.read_csv(output_path)\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(\"Sample predictions:\")\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 305 train images...\n",
      "Processing 1/305: 0013.JPG\n",
      "\n",
      "Saved predictions to /home/andrii/edth/2/gnss-denied-localization-munich/data/predictions_lightglue_train.csv\n",
      "Total predictions: 10\n",
      "Sample predictions:\n",
      "   id  x_pixel  y_pixel  gt_x_pixel  gt_y_pixel  error_distance  \\\n",
      "0  13   1991.3    697.6      2572.7       286.9           711.8   \n",
      "1  14   2261.9    779.1      2584.8       569.6           385.0   \n",
      "2  15   1991.3    992.0      2596.2       852.8           620.8   \n",
      "3  16   2548.4   1209.9      2606.5      1134.4            95.3   \n",
      "4  17   2159.2   1415.5      2616.8      1416.0           457.6   \n",
      "\n",
      "   num_tiles_used  \n",
      "0              10  \n",
      "1               7  \n",
      "2               7  \n",
      "3               3  \n",
      "4               5  \n",
      "\n",
      "Evaluation Results:\n",
      "Mean error: 384.9 pixels\n",
      "Median error: 444.1 pixels\n",
      "Std error: 226.7 pixels\n",
      "Min error: 71.0 pixels\n",
      "Max error: 711.8 pixels\n",
      "Average tiles used: 9.6\n",
      "Accuracy within 25 pixels: 0.0%\n",
      "Accuracy within 125 pixels: 30.0%\n",
      "Accuracy within 500 pixels: 70.0%\n",
      "Score: 33.3%\n"
     ]
    }
   ],
   "source": [
    "# Run on ALL train data and evaluate against ground truth\n",
    "TRAIN_IMAGE_PATH = data_path / \"train_data\" / \"train_images\"\n",
    "train_image_paths = sorted(TRAIN_IMAGE_PATH.iterdir())\n",
    "\n",
    "print(f\"Processing {len(train_image_paths)} train images...\")\n",
    "\n",
    "MIN_MATCHES_THRESHOLD = 10  # Minimum matches to consider a tile\n",
    "HIGH_CONFIDENCE_THRESHOLD = 40  # If a tile has this many matches, use only this tile\n",
    "NUM_BEST_TILES = 5\n",
    "\n",
    "# Prepare CSV file with headers\n",
    "output_path = data_path / 'predictions_lightglue_train.csv'\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write('id,x_pixel,y_pixel,gt_x_pixel,gt_y_pixel,error_distance,num_tiles_used\\n')\n",
    "\n",
    "for i, img_path in enumerate(train_image_paths):\n",
    "    if (i+1) % 11 == 0:\n",
    "        break\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing {i+1}/{len(train_image_paths)}: {img_path.name}\")\n",
    "    \n",
    "    img_id = int(img_path.name.split('.')[0])\n",
    "    \n",
    "    # Get ground truth for this image\n",
    "    gt_x, gt_y, _ = get_ground_truth_positions(train_pos, img_path)\n",
    "    \n",
    "    # Resize drone image using calculated dimensions\n",
    "    image0 = load_image(img_path, resize=(target_height, target_width)).to(device)\n",
    "    \n",
    "    # Store tile results with match counts\n",
    "    tile_results = []\n",
    "    \n",
    "    # Generate all 90-degree rotations of the image (0, 90, 180, 270 degrees)\n",
    "    rotated_images = [image0]\n",
    "    for k in range(1, 4):\n",
    "        rotated_images.append(torch.rot90(image0, k, dims=[1, 2]))\n",
    "    for rot_idx, rotated_image in enumerate(rotated_images):\n",
    "        \n",
    "        # Sliding window approach\n",
    "        windows = []\n",
    "        map_tile_positions = []\n",
    "\n",
    "        for top in range(0, MAP_HEIGHT - window_height + 1, stride_y):\n",
    "            for left in range(0, MAP_WIDTH - window_width + 1, stride_x):\n",
    "                bottom = top + window_height\n",
    "                right = left + window_width\n",
    "                window = image_map[:, top:bottom, left:right]\n",
    "                windows.append(window)\n",
    "                map_tile_positions.append(\n",
    "                    {\"top\": top,\n",
    "                    \"bottom\": bottom,\n",
    "                    \"left\": left,\n",
    "                    \"right\": right}\n",
    "                    )\n",
    "        \n",
    "        for tile_pos, tile_img in zip(map_tile_positions, windows):\n",
    "            # Extract and match to count matches without plotting\n",
    "            m_kpts0, m_kpts1, matches01, kpts0, kpts1 = extract_and_match(rotated_image, tile_img, map_features=None)\n",
    "            num_matches = len(m_kpts0)\n",
    "            \n",
    "            # Store results\n",
    "            tile_results.append({\n",
    "                'tile_pos': tile_pos,\n",
    "                'tile_img': tile_img,\n",
    "                'num_matches': num_matches,\n",
    "                'matches_data': (m_kpts0, m_kpts1, matches01, kpts0, kpts1),\n",
    "                'rotation_index': rot_idx,\n",
    "                'rotated_image': rotated_image\n",
    "            })\n",
    "        \n",
    "    # Sort by number of matches (descending) and apply filtering logic\n",
    "    sorted_tiles = sorted(tile_results, key=lambda x: x['num_matches'], reverse=True)\n",
    "\n",
    "    # Check if any tiles have high confidence\n",
    "    high_confidence_tiles = [tile for tile in sorted_tiles if tile['num_matches'] >= HIGH_CONFIDENCE_THRESHOLD]\n",
    "\n",
    "    if len(high_confidence_tiles) > 0:\n",
    "        # Use all high confidence tiles\n",
    "        best_tiles = high_confidence_tiles\n",
    "    else:\n",
    "        # Filter tiles with enough matches\n",
    "        good_tiles = [tile for tile in sorted_tiles if tile['num_matches'] >= MIN_MATCHES_THRESHOLD]\n",
    "        \n",
    "        if len(good_tiles) > 0:\n",
    "            # Use up to NUM_BEST_TILES from good tiles\n",
    "            best_tiles = good_tiles[:NUM_BEST_TILES]\n",
    "        else:\n",
    "            # If no tile meets minimum threshold, use at least the best one\n",
    "            best_tiles = [sorted_tiles[0]]\n",
    "            \n",
    "    # Process multiple best tiles and collect predictions\n",
    "    predictions_x = []\n",
    "    predictions_y = []\n",
    "    \n",
    "    for idx, tile_result in enumerate(best_tiles, 1):\n",
    "        tile_pos = tile_result['tile_pos']\n",
    "        tile_img = tile_result['tile_img']\n",
    "        num_matches = tile_result['num_matches']\n",
    "        best_rotation_idx = tile_result['rotation_index']\n",
    "        rotated_image = tile_result['rotated_image']\n",
    "        \n",
    "        # Only use tiles with at least some matches\n",
    "        if num_matches > 0:\n",
    "            pred_x, pred_y = compute_pose_prediction(rotated_image, img_path, tile_img, train_pos, \n",
    "                                          zoom_factor=1, map_features=None, plot_gt=False)\n",
    "            \n",
    "            # Adjust predicted position to full map coordinates\n",
    "            full_map_pred_x = pred_x + tile_pos['left']\n",
    "            full_map_pred_y = pred_y + tile_pos['top']\n",
    "            \n",
    "            predictions_x.append(full_map_pred_x)\n",
    "            predictions_y.append(full_map_pred_y)\n",
    "    \n",
    "    # Calculate mean prediction from multiple tiles\n",
    "    if predictions_x:\n",
    "        final_pred_x = sum(predictions_x) / len(predictions_x)\n",
    "        final_pred_y = sum(predictions_y) / len(predictions_y)\n",
    "        num_tiles_used = len(predictions_x)\n",
    "    \n",
    "    # Calculate error distance\n",
    "    error_distance = ((final_pred_x - gt_x)**2 + (final_pred_y - gt_y)**2)**0.5\n",
    "    \n",
    "    # Write prediction and evaluation to CSV immediately\n",
    "    with open(output_path, 'a') as f:\n",
    "        f.write(f'{img_id},{final_pred_x:.1f},{final_pred_y:.1f},{gt_x:.1f},{gt_y:.1f},{error_distance:.1f},{num_tiles_used}\\n')\n",
    "\n",
    "print(f\"\\nSaved predictions to {output_path}\")\n",
    "\n",
    "# Quick validation and evaluation\n",
    "predictions_df = pd.read_csv(output_path)\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(\"Sample predictions:\")\n",
    "print(predictions_df.head())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mean_error = predictions_df['error_distance'].mean()\n",
    "median_error = predictions_df['error_distance'].median()\n",
    "std_error = predictions_df['error_distance'].std()\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Mean error: {mean_error:.1f} pixels\")\n",
    "print(f\"Median error: {median_error:.1f} pixels\")\n",
    "print(f\"Std error: {std_error:.1f} pixels\")\n",
    "print(f\"Min error: {predictions_df['error_distance'].min():.1f} pixels\")\n",
    "print(f\"Max error: {predictions_df['error_distance'].max():.1f} pixels\")\n",
    "print(f\"Average tiles used: {predictions_df['num_tiles_used'].mean():.1f}\")\n",
    "\n",
    "# Accuracy at different thresholds\n",
    "thresholds = [25, 125, 500]\n",
    "accuracies = []\n",
    "for threshold in thresholds:\n",
    "    accuracy = (predictions_df['error_distance'] <= threshold).mean() * 100\n",
    "    print(f\"Accuracy within {threshold} pixels: {accuracy:.1f}%\")\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(f\"Score: {sum(accuracies)/len(accuracies):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finetune LightGlue on our dataset\n",
    "\n",
    "# Import the finetuning module\n",
    "import sys\n",
    "sys.path.append('.')  # Add current directory to path\n",
    "from finetune import (\n",
    "    DroneMapDataset, \n",
    "    LightGlueFinetuner,\n",
    "    pose_prediction_loss,\n",
    "    weighted_pose_loss,\n",
    "    augment_drone_image\n",
    ")\n",
    "\n",
    "# Enable gradients for training\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "# Prepare train/validation split\n",
    "train_indices = list(range(0, 80))  # First 80 images for training\n",
    "val_indices = list(range(80, 100))   # Last 20 images for validation\n",
    "\n",
    "train_paths = [train_image_paths[i] for i in train_indices]\n",
    "val_paths = [train_image_paths[i] for i in val_indices]\n",
    "\n",
    "print(f\"Training on {len(train_paths)} images\")\n",
    "print(f\"Validating on {len(val_paths)} images\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DroneMapDataset(\n",
    "    image_paths=train_paths,\n",
    "    train_pos=train_pos,\n",
    "    image_map=image_map,\n",
    "    target_height=target_height,\n",
    "    target_width=target_width,\n",
    "    map_height=MAP_HEIGHT,\n",
    "    map_width=MAP_WIDTH,\n",
    "    window_height=window_height,\n",
    "    window_width=window_width,\n",
    "    stride_y=stride_y,\n",
    "    stride_x=stride_x,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_dataset = DroneMapDataset(\n",
    "    image_paths=val_paths,\n",
    "    train_pos=train_pos,\n",
    "    image_map=image_map,\n",
    "    target_height=target_height,\n",
    "    target_width=target_width,\n",
    "    map_height=MAP_HEIGHT,\n",
    "    map_width=MAP_WIDTH,\n",
    "    window_height=window_height,\n",
    "    window_width=window_width,\n",
    "    stride_y=stride_y,\n",
    "    stride_x=stride_x,\n",
    "    augment=False  # No augmentation for validation\n",
    ")\n",
    "\n",
    "# Initialize finetuner\n",
    "finetuner = LightGlueFinetuner(\n",
    "    extractor=extractor,\n",
    "    matcher=matcher,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Finetune the model\n",
    "print(\"Starting finetuning...\")\n",
    "history = finetuner.finetune(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    num_epochs=5,\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=1,  # Keep batch size 1 for simplicity\n",
    "    save_path='lightglue_finetuned.pth'\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot losses\n",
    "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (pixels)')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot errors\n",
    "ax2.plot(history['train_error'], label='Train Error', marker='o')\n",
    "ax2.plot(history['val_error'], label='Val Error', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Error (pixels)')\n",
    "ax2.set_title('Training and Validation Error')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "print(f\"\\nFinetuning completed!\")\n",
    "print(f\"Final train loss: {history['train_loss'][-1]:.1f} pixels\")\n",
    "print(f\"Final val loss: {history['val_loss'][-1]:.1f} pixels\")\n",
    "print(f\"Final train error: {history['train_error'][-1]:.1f} pixels\")\n",
    "print(f\"Final val error: {history['val_error'][-1]:.1f} pixels\")\n",
    "\n",
    "# Disable gradients again for inference\n",
    "torch.set_grad_enabled(False)\n",
    "matcher.eval()\n",
    "\n",
    "print(\"Model is now finetuned and ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy example\n",
    "The top image shows the matches, while the bottom image shows the point pruning across layers. In this case, LightGlue prunes a few points with occlusions, but is able to stop the context aggregation after 4/9 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image0 = load_image(images / \"DSC_0411.JPG\")\n",
    "image1 = load_image(images / \"DSC_0410.JPG\")\n",
    "\n",
    "feats0 = extractor.extract(image0.to(device))\n",
    "feats1 = extractor.extract(image1.to(device))\n",
    "matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n",
    "feats0, feats1, matches01 = [\n",
    "    rbd(x) for x in [feats0, feats1, matches01]\n",
    "]  # remove batch dimension\n",
    "\n",
    "kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "\n",
    "axes = viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_matches(m_kpts0, m_kpts1, color=\"lime\", lw=0.2)\n",
    "viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers', fs=20)\n",
    "\n",
    "kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficult example\n",
    "For pairs with significant viewpoint- and illumination changes, LightGlue can exclude a lot of points early in the matching process (red points), which significantly reduces the inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image0 = load_image(images / \"sacre_coeur1.jpg\")\n",
    "image1 = load_image(images / \"sacre_coeur2.jpg\")\n",
    "\n",
    "feats0 = extractor.extract(image0.to(device))\n",
    "feats1 = extractor.extract(image1.to(device))\n",
    "matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n",
    "feats0, feats1, matches01 = [\n",
    "    rbd(x) for x in [feats0, feats1, matches01]\n",
    "]  # remove batch dimension\n",
    "\n",
    "kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "\n",
    "axes = viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_matches(m_kpts0, m_kpts1, color=\"lime\", lw=0.2)\n",
    "viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers')\n",
    "\n",
    "kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".se3loc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
